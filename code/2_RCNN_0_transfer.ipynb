{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /mnt/e/py_data/project_3_git\n"
     ]
    }
   ],
   "source": [
    "# 기본 작업 경로 설정\n",
    "\n",
    "import os\n",
    "notebook_path = os.path.abspath(\"../readme.md\")\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# 현재 작업 디렉토리 출력\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# 데이터셋 경로와 형식 설정\n",
    "register_coco_instances(\"face_data_set\", {}, \"data/ssd_rcnn_face/annotations.json\", \"data/yolo_data/train/\")\n",
    "register_coco_instances(\"face_data_set_valid\", {}, \"data/ssd_rcnn_face/annotations_val.json\", \"data/yolo_data/val/\")\n",
    "\n",
    "# 메타데이터를 확인.\n",
    "metadata = MetadataCatalog.get(\"face_data_set\")\n",
    "dataset_dicts = DatasetCatalog.get(\"face_data_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config import CfgNode as CN\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.model_zoo import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "'''\n",
    "각 하이퍼 파라미터 설명\n",
    "cfg.DATASETS.TRAIN : 학습에 사용할 데이터셋을 설정.\n",
    "cfg.DATASETS.TEST : 테스트에 사용할 데이터셋을 설정.\n",
    "cfg.DATALOADER.NUM_WORKERS : 데이터로더의 워커 수를 설정.\n",
    "cfg.MODEL.WEIGHTS : 모델의 가중치를 설정.\n",
    "cfg.SOLVER.IMS_PER_BATCH : 배치 크기를 설정.\n",
    "cfg.SOLVER.BASE_LR : 학습률을 설정.\n",
    "cfg.SOLVER.MAX_ITER : 최대 반복 횟수를 설정.\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE : 이미지 당 ROI 배치 크기를 설정.\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES : 클래스 수를 설정. (배경 클래스 포함 하지 않음)\n",
    "cfg.OUTPUT_DIR : 모델의 저장 경로를 설정.\n",
    "\n",
    "추가 하이퍼 파라미터\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD : 체크포인트 저장 주기를 설정.\n",
    "cfg.TEST.EVAL_PERIOD : 평가 주기를 설정.\n",
    "'''\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def train(self):\n",
    "        super().train()\n",
    "        # Add evaluation after each epoch\n",
    "        for epoch in range(self.start_iter, self.max_iter + 1):\n",
    "            if epoch % self.cfg.TEST.EVAL_PERIOD == 0:\n",
    "                self.do_test()\n",
    "            if epoch % self.cfg.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "                self.checkpointer.save(f\"model_{epoch}\")\n",
    "\n",
    "    def do_test(self):\n",
    "        # Perform evaluation\n",
    "        evaluator = COCOEvaluator(self.cfg.DATASETS.TEST[0], output_dir=self.cfg.OUTPUT_DIR)\n",
    "        val_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])\n",
    "        results = inference_on_dataset(self.model, val_loader, evaluator)\n",
    "        print(results)\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "# 다양한 백본 설정 가능\n",
    "# cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "# cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
    "# cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_C4_3x.yaml\")\n",
    "# ... 등 등\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"face_data_set\",)\n",
    "cfg.DATASETS.TEST = (\"face_data_set_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 100\n",
    "\n",
    "# 입력 사이즈 설정\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "\n",
    "# 간단한 증강\n",
    "cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.INPUT.RANDOM_ROTATION = 30  # 최대 30도 회전\n",
    "cfg.INPUT.CROP = CN({\"ENABLED\": True, \"TYPE\": \"relative_range\", \"SIZE\": [0.8, 0.8]})\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # 사용자 클래스 수로 설정\n",
    "cfg.OUTPUT_DIR = \"./models/faster_rcnn_R_50_FPN_3x\"\n",
    "\n",
    "# 검증 평가 주기\n",
    "cfg.TEST.EVAL_PERIOD = 100  # 매 100 iterations마다 평가\n",
    "\n",
    "# 체크포인트 저장 주기\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500  # 매 500 iterations마다 체크포인트 저장\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"  # 또는 \"WarmupMultiStepLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500  # Warmup iterations 설정\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 14:32:48 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[09/06 14:33:19 d2.data.datasets.coco]: \u001b[0mLoading data/ssd_rcnn_face/annotations.json takes 30.50 seconds.\n",
      "\u001b[32m[09/06 14:33:19 d2.data.datasets.coco]: \u001b[0mLoaded 5992 images in COCO format from data/ssd_rcnn_face/annotations.json\n",
      "\u001b[32m[09/06 14:33:19 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 5992 images left.\n",
      "\u001b[32m[09/06 14:33:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='relative_range', crop_size=[0.8, 0.8]), ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[09/06 14:33:19 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[09/06 14:33:19 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 14:33:19 d2.data.common]: \u001b[0mSerializing 5992 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 14:33:19 d2.data.common]: \u001b[0mSerialized dataset takes 1.79 MiB\n",
      "\u001b[32m[09/06 14:33:19 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=16\n",
      "\u001b[32m[09/06 14:33:19 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn4.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 14:33:19 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[09/06 14:33:46 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 14:33:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 14:33:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 14:33:46 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 14:33:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 14:33:46 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 14:33:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 14:33:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0483 s/iter. Inference: 0.0814 s/iter. Eval: 0.0007 s/iter. Total: 0.1305 s/iter. ETA=0:02:35\n",
      "\u001b[32m[09/06 14:33:53 d2.evaluation.evaluator]: \u001b[0mInference done 49/1199. Dataloading: 0.0647 s/iter. Inference: 0.0666 s/iter. Eval: 0.0007 s/iter. Total: 0.1321 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/06 14:33:59 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0837 s/iter. Inference: 0.0674 s/iter. Eval: 0.0008 s/iter. Total: 0.1519 s/iter. ETA=0:02:49\n",
      "\u001b[32m[09/06 14:34:04 d2.evaluation.evaluator]: \u001b[0mInference done 118/1199. Dataloading: 0.0810 s/iter. Inference: 0.0702 s/iter. Eval: 0.0008 s/iter. Total: 0.1521 s/iter. ETA=0:02:44\n",
      "\u001b[32m[09/06 14:34:09 d2.evaluation.evaluator]: \u001b[0mInference done 147/1199. Dataloading: 0.0862 s/iter. Inference: 0.0693 s/iter. Eval: 0.0008 s/iter. Total: 0.1564 s/iter. ETA=0:02:44\n",
      "\u001b[32m[09/06 14:34:15 d2.evaluation.evaluator]: \u001b[0mInference done 183/1199. Dataloading: 0.0845 s/iter. Inference: 0.0684 s/iter. Eval: 0.0008 s/iter. Total: 0.1537 s/iter. ETA=0:02:36\n",
      "\u001b[32m[09/06 14:34:20 d2.evaluation.evaluator]: \u001b[0mInference done 215/1199. Dataloading: 0.0843 s/iter. Inference: 0.0692 s/iter. Eval: 0.0008 s/iter. Total: 0.1543 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/06 14:34:25 d2.evaluation.evaluator]: \u001b[0mInference done 241/1199. Dataloading: 0.0881 s/iter. Inference: 0.0712 s/iter. Eval: 0.0008 s/iter. Total: 0.1602 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/06 14:34:30 d2.evaluation.evaluator]: \u001b[0mInference done 270/1199. Dataloading: 0.0901 s/iter. Inference: 0.0718 s/iter. Eval: 0.0008 s/iter. Total: 0.1628 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/06 14:34:35 d2.evaluation.evaluator]: \u001b[0mInference done 286/1199. Dataloading: 0.0986 s/iter. Inference: 0.0725 s/iter. Eval: 0.0008 s/iter. Total: 0.1720 s/iter. ETA=0:02:37\n",
      "\u001b[32m[09/06 14:34:41 d2.evaluation.evaluator]: \u001b[0mInference done 316/1199. Dataloading: 0.0977 s/iter. Inference: 0.0730 s/iter. Eval: 0.0008 s/iter. Total: 0.1717 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/06 14:34:46 d2.evaluation.evaluator]: \u001b[0mInference done 352/1199. Dataloading: 0.0961 s/iter. Inference: 0.0718 s/iter. Eval: 0.0008 s/iter. Total: 0.1687 s/iter. ETA=0:02:22\n",
      "\u001b[32m[09/06 14:34:51 d2.evaluation.evaluator]: \u001b[0mInference done 383/1199. Dataloading: 0.0960 s/iter. Inference: 0.0713 s/iter. Eval: 0.0008 s/iter. Total: 0.1682 s/iter. ETA=0:02:17\n",
      "\u001b[32m[09/06 14:34:56 d2.evaluation.evaluator]: \u001b[0mInference done 413/1199. Dataloading: 0.0959 s/iter. Inference: 0.0713 s/iter. Eval: 0.0008 s/iter. Total: 0.1681 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/06 14:35:01 d2.evaluation.evaluator]: \u001b[0mInference done 448/1199. Dataloading: 0.0939 s/iter. Inference: 0.0713 s/iter. Eval: 0.0008 s/iter. Total: 0.1661 s/iter. ETA=0:02:04\n",
      "\u001b[32m[09/06 14:35:06 d2.evaluation.evaluator]: \u001b[0mInference done 483/1199. Dataloading: 0.0927 s/iter. Inference: 0.0710 s/iter. Eval: 0.0008 s/iter. Total: 0.1646 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/06 14:35:11 d2.evaluation.evaluator]: \u001b[0mInference done 516/1199. Dataloading: 0.0921 s/iter. Inference: 0.0709 s/iter. Eval: 0.0008 s/iter. Total: 0.1639 s/iter. ETA=0:01:51\n",
      "\u001b[32m[09/06 14:35:16 d2.evaluation.evaluator]: \u001b[0mInference done 550/1199. Dataloading: 0.0915 s/iter. Inference: 0.0705 s/iter. Eval: 0.0008 s/iter. Total: 0.1629 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/06 14:35:21 d2.evaluation.evaluator]: \u001b[0mInference done 581/1199. Dataloading: 0.0908 s/iter. Inference: 0.0712 s/iter. Eval: 0.0008 s/iter. Total: 0.1630 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/06 14:35:26 d2.evaluation.evaluator]: \u001b[0mInference done 612/1199. Dataloading: 0.0906 s/iter. Inference: 0.0714 s/iter. Eval: 0.0009 s/iter. Total: 0.1630 s/iter. ETA=0:01:35\n",
      "\u001b[32m[09/06 14:35:31 d2.evaluation.evaluator]: \u001b[0mInference done 641/1199. Dataloading: 0.0912 s/iter. Inference: 0.0715 s/iter. Eval: 0.0009 s/iter. Total: 0.1637 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/06 14:35:36 d2.evaluation.evaluator]: \u001b[0mInference done 665/1199. Dataloading: 0.0925 s/iter. Inference: 0.0721 s/iter. Eval: 0.0008 s/iter. Total: 0.1656 s/iter. ETA=0:01:28\n",
      "\u001b[32m[09/06 14:35:41 d2.evaluation.evaluator]: \u001b[0mInference done 691/1199. Dataloading: 0.0929 s/iter. Inference: 0.0729 s/iter. Eval: 0.0008 s/iter. Total: 0.1667 s/iter. ETA=0:01:24\n",
      "\u001b[32m[09/06 14:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 722/1199. Dataloading: 0.0925 s/iter. Inference: 0.0731 s/iter. Eval: 0.0008 s/iter. Total: 0.1665 s/iter. ETA=0:01:19\n",
      "\u001b[32m[09/06 14:35:52 d2.evaluation.evaluator]: \u001b[0mInference done 747/1199. Dataloading: 0.0939 s/iter. Inference: 0.0734 s/iter. Eval: 0.0008 s/iter. Total: 0.1682 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/06 14:35:57 d2.evaluation.evaluator]: \u001b[0mInference done 768/1199. Dataloading: 0.0955 s/iter. Inference: 0.0737 s/iter. Eval: 0.0008 s/iter. Total: 0.1702 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/06 14:36:02 d2.evaluation.evaluator]: \u001b[0mInference done 801/1199. Dataloading: 0.0952 s/iter. Inference: 0.0735 s/iter. Eval: 0.0008 s/iter. Total: 0.1696 s/iter. ETA=0:01:07\n",
      "\u001b[32m[09/06 14:36:07 d2.evaluation.evaluator]: \u001b[0mInference done 828/1199. Dataloading: 0.0945 s/iter. Inference: 0.0740 s/iter. Eval: 0.0015 s/iter. Total: 0.1701 s/iter. ETA=0:01:03\n",
      "\u001b[32m[09/06 14:36:12 d2.evaluation.evaluator]: \u001b[0mInference done 863/1199. Dataloading: 0.0938 s/iter. Inference: 0.0738 s/iter. Eval: 0.0014 s/iter. Total: 0.1691 s/iter. ETA=0:00:56\n",
      "\u001b[32m[09/06 14:36:17 d2.evaluation.evaluator]: \u001b[0mInference done 895/1199. Dataloading: 0.0936 s/iter. Inference: 0.0736 s/iter. Eval: 0.0014 s/iter. Total: 0.1687 s/iter. ETA=0:00:51\n",
      "\u001b[32m[09/06 14:36:22 d2.evaluation.evaluator]: \u001b[0mInference done 927/1199. Dataloading: 0.0936 s/iter. Inference: 0.0734 s/iter. Eval: 0.0014 s/iter. Total: 0.1685 s/iter. ETA=0:00:45\n",
      "\u001b[32m[09/06 14:36:28 d2.evaluation.evaluator]: \u001b[0mInference done 961/1199. Dataloading: 0.0931 s/iter. Inference: 0.0732 s/iter. Eval: 0.0013 s/iter. Total: 0.1677 s/iter. ETA=0:00:39\n",
      "\u001b[32m[09/06 14:36:33 d2.evaluation.evaluator]: \u001b[0mInference done 987/1199. Dataloading: 0.0938 s/iter. Inference: 0.0732 s/iter. Eval: 0.0013 s/iter. Total: 0.1684 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/06 14:36:39 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0948 s/iter. Inference: 0.0731 s/iter. Eval: 0.0013 s/iter. Total: 0.1693 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/06 14:36:44 d2.evaluation.evaluator]: \u001b[0mInference done 1054/1199. Dataloading: 0.0941 s/iter. Inference: 0.0730 s/iter. Eval: 0.0013 s/iter. Total: 0.1685 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/06 14:36:49 d2.evaluation.evaluator]: \u001b[0mInference done 1083/1199. Dataloading: 0.0943 s/iter. Inference: 0.0730 s/iter. Eval: 0.0013 s/iter. Total: 0.1687 s/iter. ETA=0:00:19\n",
      "\u001b[32m[09/06 14:36:54 d2.evaluation.evaluator]: \u001b[0mInference done 1117/1199. Dataloading: 0.0938 s/iter. Inference: 0.0728 s/iter. Eval: 0.0012 s/iter. Total: 0.1680 s/iter. ETA=0:00:13\n",
      "\u001b[32m[09/06 14:36:59 d2.evaluation.evaluator]: \u001b[0mInference done 1143/1199. Dataloading: 0.0945 s/iter. Inference: 0.0727 s/iter. Eval: 0.0012 s/iter. Total: 0.1686 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/06 14:37:04 d2.evaluation.evaluator]: \u001b[0mInference done 1176/1199. Dataloading: 0.0941 s/iter. Inference: 0.0728 s/iter. Eval: 0.0012 s/iter. Total: 0.1682 s/iter. ETA=0:00:03\n",
      "\u001b[32m[09/06 14:37:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:20.415847 (0.167852 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 14:37:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:26 (0.072611 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 14:37:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 14:37:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 14:37:08 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.50 seconds.\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.13 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.024\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.002 | 0.011  | 0.000  |  nan  |  nan  | 0.003 |\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|:-----------|:------|\n",
      "| angry      | 0.000 | sad        | 0.010 | surprised  | 0.000 |\n",
      "| happy      | 0.000 |            |       |            |       |\n",
      "\u001b[32m[09/06 14:37:09 d2.engine.defaults]: \u001b[0mEvaluation results for face_data_set_valid in csv format:\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[09/06 14:37:09 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0024,0.0109,0.0002,nan,nan,0.0029\n",
      "\u001b[32m[09/06 14:37:37 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 14:37:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 14:37:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 14:37:37 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 14:37:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 14:37:37 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 14:37:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 14:37:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0388 s/iter. Inference: 0.0599 s/iter. Eval: 0.0008 s/iter. Total: 0.0996 s/iter. ETA=0:01:58\n",
      "\u001b[32m[09/06 14:37:44 d2.evaluation.evaluator]: \u001b[0mInference done 50/1199. Dataloading: 0.0659 s/iter. Inference: 0.0595 s/iter. Eval: 0.0008 s/iter. Total: 0.1264 s/iter. ETA=0:02:25\n",
      "\u001b[32m[09/06 14:37:49 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0830 s/iter. Inference: 0.0603 s/iter. Eval: 0.0007 s/iter. Total: 0.1441 s/iter. ETA=0:02:40\n",
      "\u001b[32m[09/06 14:37:55 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0858 s/iter. Inference: 0.0652 s/iter. Eval: 0.0007 s/iter. Total: 0.1518 s/iter. ETA=0:02:43\n",
      "\u001b[32m[09/06 14:38:00 d2.evaluation.evaluator]: \u001b[0mInference done 157/1199. Dataloading: 0.0816 s/iter. Inference: 0.0650 s/iter. Eval: 0.0007 s/iter. Total: 0.1474 s/iter. ETA=0:02:33\n",
      "\u001b[32m[09/06 14:38:05 d2.evaluation.evaluator]: \u001b[0mInference done 191/1199. Dataloading: 0.0801 s/iter. Inference: 0.0669 s/iter. Eval: 0.0007 s/iter. Total: 0.1478 s/iter. ETA=0:02:29\n",
      "\u001b[32m[09/06 14:38:11 d2.evaluation.evaluator]: \u001b[0mInference done 221/1199. Dataloading: 0.0833 s/iter. Inference: 0.0673 s/iter. Eval: 0.0007 s/iter. Total: 0.1514 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/06 14:38:16 d2.evaluation.evaluator]: \u001b[0mInference done 248/1199. Dataloading: 0.0857 s/iter. Inference: 0.0689 s/iter. Eval: 0.0007 s/iter. Total: 0.1554 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/06 14:38:21 d2.evaluation.evaluator]: \u001b[0mInference done 276/1199. Dataloading: 0.0883 s/iter. Inference: 0.0691 s/iter. Eval: 0.0007 s/iter. Total: 0.1582 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/06 14:38:26 d2.evaluation.evaluator]: \u001b[0mInference done 298/1199. Dataloading: 0.0942 s/iter. Inference: 0.0693 s/iter. Eval: 0.0007 s/iter. Total: 0.1643 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/06 14:38:31 d2.evaluation.evaluator]: \u001b[0mInference done 335/1199. Dataloading: 0.0916 s/iter. Inference: 0.0690 s/iter. Eval: 0.0007 s/iter. Total: 0.1614 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/06 14:38:36 d2.evaluation.evaluator]: \u001b[0mInference done 370/1199. Dataloading: 0.0903 s/iter. Inference: 0.0690 s/iter. Eval: 0.0007 s/iter. Total: 0.1601 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/06 14:38:41 d2.evaluation.evaluator]: \u001b[0mInference done 402/1199. Dataloading: 0.0900 s/iter. Inference: 0.0695 s/iter. Eval: 0.0007 s/iter. Total: 0.1602 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/06 14:38:46 d2.evaluation.evaluator]: \u001b[0mInference done 435/1199. Dataloading: 0.0893 s/iter. Inference: 0.0695 s/iter. Eval: 0.0008 s/iter. Total: 0.1597 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/06 14:38:52 d2.evaluation.evaluator]: \u001b[0mInference done 469/1199. Dataloading: 0.0882 s/iter. Inference: 0.0700 s/iter. Eval: 0.0008 s/iter. Total: 0.1591 s/iter. ETA=0:01:56\n",
      "\u001b[32m[09/06 14:38:57 d2.evaluation.evaluator]: \u001b[0mInference done 507/1199. Dataloading: 0.0871 s/iter. Inference: 0.0692 s/iter. Eval: 0.0008 s/iter. Total: 0.1571 s/iter. ETA=0:01:48\n",
      "\u001b[32m[09/06 14:39:02 d2.evaluation.evaluator]: \u001b[0mInference done 540/1199. Dataloading: 0.0869 s/iter. Inference: 0.0691 s/iter. Eval: 0.0008 s/iter. Total: 0.1569 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/06 14:39:07 d2.evaluation.evaluator]: \u001b[0mInference done 577/1199. Dataloading: 0.0856 s/iter. Inference: 0.0691 s/iter. Eval: 0.0008 s/iter. Total: 0.1556 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/06 14:39:12 d2.evaluation.evaluator]: \u001b[0mInference done 614/1199. Dataloading: 0.0847 s/iter. Inference: 0.0689 s/iter. Eval: 0.0008 s/iter. Total: 0.1545 s/iter. ETA=0:01:30\n",
      "\u001b[32m[09/06 14:39:18 d2.evaluation.evaluator]: \u001b[0mInference done 645/1199. Dataloading: 0.0868 s/iter. Inference: 0.0687 s/iter. Eval: 0.0008 s/iter. Total: 0.1564 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/06 14:39:23 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.0874 s/iter. Inference: 0.0687 s/iter. Eval: 0.0008 s/iter. Total: 0.1570 s/iter. ETA=0:01:22\n",
      "\u001b[32m[09/06 14:39:28 d2.evaluation.evaluator]: \u001b[0mInference done 715/1199. Dataloading: 0.0857 s/iter. Inference: 0.0687 s/iter. Eval: 0.0008 s/iter. Total: 0.1553 s/iter. ETA=0:01:15\n",
      "\u001b[32m[09/06 14:39:33 d2.evaluation.evaluator]: \u001b[0mInference done 747/1199. Dataloading: 0.0865 s/iter. Inference: 0.0685 s/iter. Eval: 0.0008 s/iter. Total: 0.1559 s/iter. ETA=0:01:10\n",
      "\u001b[32m[09/06 14:39:39 d2.evaluation.evaluator]: \u001b[0mInference done 775/1199. Dataloading: 0.0873 s/iter. Inference: 0.0686 s/iter. Eval: 0.0008 s/iter. Total: 0.1568 s/iter. ETA=0:01:06\n",
      "\u001b[32m[09/06 14:39:44 d2.evaluation.evaluator]: \u001b[0mInference done 815/1199. Dataloading: 0.0863 s/iter. Inference: 0.0681 s/iter. Eval: 0.0008 s/iter. Total: 0.1553 s/iter. ETA=0:00:59\n",
      "\u001b[32m[09/06 14:39:49 d2.evaluation.evaluator]: \u001b[0mInference done 856/1199. Dataloading: 0.0854 s/iter. Inference: 0.0677 s/iter. Eval: 0.0008 s/iter. Total: 0.1540 s/iter. ETA=0:00:52\n",
      "\u001b[32m[09/06 14:39:54 d2.evaluation.evaluator]: \u001b[0mInference done 891/1199. Dataloading: 0.0847 s/iter. Inference: 0.0680 s/iter. Eval: 0.0008 s/iter. Total: 0.1536 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/06 14:39:59 d2.evaluation.evaluator]: \u001b[0mInference done 924/1199. Dataloading: 0.0848 s/iter. Inference: 0.0679 s/iter. Eval: 0.0008 s/iter. Total: 0.1535 s/iter. ETA=0:00:42\n",
      "\u001b[32m[09/06 14:40:05 d2.evaluation.evaluator]: \u001b[0mInference done 963/1199. Dataloading: 0.0850 s/iter. Inference: 0.0677 s/iter. Eval: 0.0008 s/iter. Total: 0.1535 s/iter. ETA=0:00:36\n",
      "\u001b[32m[09/06 14:40:10 d2.evaluation.evaluator]: \u001b[0mInference done 996/1199. Dataloading: 0.0849 s/iter. Inference: 0.0678 s/iter. Eval: 0.0007 s/iter. Total: 0.1535 s/iter. ETA=0:00:31\n",
      "\u001b[32m[09/06 14:40:15 d2.evaluation.evaluator]: \u001b[0mInference done 1025/1199. Dataloading: 0.0854 s/iter. Inference: 0.0679 s/iter. Eval: 0.0007 s/iter. Total: 0.1541 s/iter. ETA=0:00:26\n",
      "\u001b[32m[09/06 14:40:20 d2.evaluation.evaluator]: \u001b[0mInference done 1058/1199. Dataloading: 0.0851 s/iter. Inference: 0.0681 s/iter. Eval: 0.0008 s/iter. Total: 0.1541 s/iter. ETA=0:00:21\n",
      "\u001b[32m[09/06 14:40:25 d2.evaluation.evaluator]: \u001b[0mInference done 1091/1199. Dataloading: 0.0852 s/iter. Inference: 0.0680 s/iter. Eval: 0.0008 s/iter. Total: 0.1541 s/iter. ETA=0:00:16\n",
      "\u001b[32m[09/06 14:40:31 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.0854 s/iter. Inference: 0.0679 s/iter. Eval: 0.0007 s/iter. Total: 0.1541 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/06 14:40:36 d2.evaluation.evaluator]: \u001b[0mInference done 1161/1199. Dataloading: 0.0852 s/iter. Inference: 0.0679 s/iter. Eval: 0.0007 s/iter. Total: 0.1540 s/iter. ETA=0:00:05\n",
      "\u001b[32m[09/06 14:40:41 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:03.038232 (0.153298 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 14:40:41 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:20 (0.067704 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 14:40:41 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 14:40:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 14:40:41 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.50 seconds.\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.13 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.025\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.003 | 0.012  | 0.000  |  nan  |  nan  | 0.003 |\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|:-----------|:------|\n",
      "| angry      | 0.000 | sad        | 0.010 | surprised  | 0.000 |\n",
      "| happy      | 0.000 |            |       |            |       |\n",
      "\u001b[32m[09/06 14:40:42 d2.engine.defaults]: \u001b[0mEvaluation results for face_data_set_valid in csv format:\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[09/06 14:40:42 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0026,0.0115,0.0002,nan,nan,0.0032\n",
      "\u001b[32m[09/06 14:40:42 d2.utils.events]: \u001b[0m eta: 0:03:33  iter: 19  total_loss: 1.757  loss_cls: 1.523  loss_box_reg: 0.1337  loss_rpn_cls: 0.08761  loss_rpn_loc: 0.01091    time: 2.6837  last_time: 2.6885  data_time: 1.6358  last_data_time: 1.6711   lr: 8.1e-07  max_mem: 10128M\n",
      "\u001b[32m[09/06 14:41:08 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 14:41:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 14:41:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 14:41:08 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 14:41:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 14:41:08 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 14:41:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 14:41:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0482 s/iter. Inference: 0.0612 s/iter. Eval: 0.0007 s/iter. Total: 0.1101 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/06 14:41:15 d2.evaluation.evaluator]: \u001b[0mInference done 50/1199. Dataloading: 0.0652 s/iter. Inference: 0.0628 s/iter. Eval: 0.0007 s/iter. Total: 0.1289 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/06 14:41:21 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0848 s/iter. Inference: 0.0632 s/iter. Eval: 0.0007 s/iter. Total: 0.1488 s/iter. ETA=0:02:45\n",
      "\u001b[32m[09/06 14:41:27 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0881 s/iter. Inference: 0.0642 s/iter. Eval: 0.0007 s/iter. Total: 0.1531 s/iter. ETA=0:02:45\n",
      "\u001b[32m[09/06 14:41:32 d2.evaluation.evaluator]: \u001b[0mInference done 159/1199. Dataloading: 0.0827 s/iter. Inference: 0.0633 s/iter. Eval: 0.0007 s/iter. Total: 0.1468 s/iter. ETA=0:02:32\n",
      "\u001b[32m[09/06 14:41:37 d2.evaluation.evaluator]: \u001b[0mInference done 195/1199. Dataloading: 0.0821 s/iter. Inference: 0.0633 s/iter. Eval: 0.0007 s/iter. Total: 0.1462 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/06 14:41:43 d2.evaluation.evaluator]: \u001b[0mInference done 227/1199. Dataloading: 0.0837 s/iter. Inference: 0.0644 s/iter. Eval: 0.0007 s/iter. Total: 0.1489 s/iter. ETA=0:02:24\n",
      "\u001b[32m[09/06 14:41:48 d2.evaluation.evaluator]: \u001b[0mInference done 259/1199. Dataloading: 0.0843 s/iter. Inference: 0.0650 s/iter. Eval: 0.0007 s/iter. Total: 0.1501 s/iter. ETA=0:02:21\n",
      "\u001b[32m[09/06 14:41:54 d2.evaluation.evaluator]: \u001b[0mInference done 282/1199. Dataloading: 0.0937 s/iter. Inference: 0.0656 s/iter. Eval: 0.0007 s/iter. Total: 0.1600 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/06 14:41:59 d2.evaluation.evaluator]: \u001b[0mInference done 315/1199. Dataloading: 0.0923 s/iter. Inference: 0.0661 s/iter. Eval: 0.0007 s/iter. Total: 0.1592 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/06 14:42:04 d2.evaluation.evaluator]: \u001b[0mInference done 350/1199. Dataloading: 0.0907 s/iter. Inference: 0.0661 s/iter. Eval: 0.0007 s/iter. Total: 0.1576 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/06 14:42:09 d2.evaluation.evaluator]: \u001b[0mInference done 382/1199. Dataloading: 0.0907 s/iter. Inference: 0.0663 s/iter. Eval: 0.0007 s/iter. Total: 0.1577 s/iter. ETA=0:02:08\n",
      "\u001b[32m[09/06 14:42:14 d2.evaluation.evaluator]: \u001b[0mInference done 412/1199. Dataloading: 0.0908 s/iter. Inference: 0.0674 s/iter. Eval: 0.0007 s/iter. Total: 0.1590 s/iter. ETA=0:02:05\n",
      "\u001b[32m[09/06 14:42:19 d2.evaluation.evaluator]: \u001b[0mInference done 451/1199. Dataloading: 0.0883 s/iter. Inference: 0.0674 s/iter. Eval: 0.0007 s/iter. Total: 0.1566 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/06 14:42:24 d2.evaluation.evaluator]: \u001b[0mInference done 484/1199. Dataloading: 0.0880 s/iter. Inference: 0.0676 s/iter. Eval: 0.0007 s/iter. Total: 0.1564 s/iter. ETA=0:01:51\n",
      "\u001b[32m[09/06 14:42:29 d2.evaluation.evaluator]: \u001b[0mInference done 520/1199. Dataloading: 0.0868 s/iter. Inference: 0.0677 s/iter. Eval: 0.0007 s/iter. Total: 0.1553 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/06 14:42:35 d2.evaluation.evaluator]: \u001b[0mInference done 554/1199. Dataloading: 0.0865 s/iter. Inference: 0.0677 s/iter. Eval: 0.0007 s/iter. Total: 0.1551 s/iter. ETA=0:01:40\n",
      "\u001b[32m[09/06 14:42:40 d2.evaluation.evaluator]: \u001b[0mInference done 592/1199. Dataloading: 0.0859 s/iter. Inference: 0.0674 s/iter. Eval: 0.0007 s/iter. Total: 0.1541 s/iter. ETA=0:01:33\n",
      "\u001b[32m[09/06 14:42:45 d2.evaluation.evaluator]: \u001b[0mInference done 632/1199. Dataloading: 0.0848 s/iter. Inference: 0.0670 s/iter. Eval: 0.0007 s/iter. Total: 0.1526 s/iter. ETA=0:01:26\n",
      "\u001b[32m[09/06 14:42:50 d2.evaluation.evaluator]: \u001b[0mInference done 661/1199. Dataloading: 0.0859 s/iter. Inference: 0.0671 s/iter. Eval: 0.0007 s/iter. Total: 0.1538 s/iter. ETA=0:01:22\n",
      "\u001b[32m[09/06 14:42:55 d2.evaluation.evaluator]: \u001b[0mInference done 688/1199. Dataloading: 0.0866 s/iter. Inference: 0.0677 s/iter. Eval: 0.0007 s/iter. Total: 0.1552 s/iter. ETA=0:01:19\n",
      "\u001b[32m[09/06 14:43:00 d2.evaluation.evaluator]: \u001b[0mInference done 720/1199. Dataloading: 0.0862 s/iter. Inference: 0.0682 s/iter. Eval: 0.0007 s/iter. Total: 0.1553 s/iter. ETA=0:01:14\n",
      "\u001b[32m[09/06 14:43:06 d2.evaluation.evaluator]: \u001b[0mInference done 748/1199. Dataloading: 0.0872 s/iter. Inference: 0.0682 s/iter. Eval: 0.0007 s/iter. Total: 0.1562 s/iter. ETA=0:01:10\n",
      "\u001b[32m[09/06 14:43:11 d2.evaluation.evaluator]: \u001b[0mInference done 772/1199. Dataloading: 0.0886 s/iter. Inference: 0.0686 s/iter. Eval: 0.0007 s/iter. Total: 0.1580 s/iter. ETA=0:01:07\n",
      "\u001b[32m[09/06 14:43:16 d2.evaluation.evaluator]: \u001b[0mInference done 807/1199. Dataloading: 0.0879 s/iter. Inference: 0.0687 s/iter. Eval: 0.0007 s/iter. Total: 0.1574 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/06 14:43:21 d2.evaluation.evaluator]: \u001b[0mInference done 848/1199. Dataloading: 0.0867 s/iter. Inference: 0.0683 s/iter. Eval: 0.0007 s/iter. Total: 0.1558 s/iter. ETA=0:00:54\n",
      "\u001b[32m[09/06 14:43:26 d2.evaluation.evaluator]: \u001b[0mInference done 883/1199. Dataloading: 0.0864 s/iter. Inference: 0.0681 s/iter. Eval: 0.0007 s/iter. Total: 0.1553 s/iter. ETA=0:00:49\n",
      "\u001b[32m[09/06 14:43:31 d2.evaluation.evaluator]: \u001b[0mInference done 915/1199. Dataloading: 0.0866 s/iter. Inference: 0.0680 s/iter. Eval: 0.0007 s/iter. Total: 0.1553 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/06 14:43:36 d2.evaluation.evaluator]: \u001b[0mInference done 957/1199. Dataloading: 0.0854 s/iter. Inference: 0.0677 s/iter. Eval: 0.0007 s/iter. Total: 0.1539 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/06 14:43:41 d2.evaluation.evaluator]: \u001b[0mInference done 985/1199. Dataloading: 0.0858 s/iter. Inference: 0.0680 s/iter. Eval: 0.0007 s/iter. Total: 0.1547 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/06 14:43:47 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.0865 s/iter. Inference: 0.0678 s/iter. Eval: 0.0007 s/iter. Total: 0.1551 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/06 14:43:52 d2.evaluation.evaluator]: \u001b[0mInference done 1052/1199. Dataloading: 0.0864 s/iter. Inference: 0.0679 s/iter. Eval: 0.0007 s/iter. Total: 0.1551 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/06 14:43:57 d2.evaluation.evaluator]: \u001b[0mInference done 1081/1199. Dataloading: 0.0867 s/iter. Inference: 0.0681 s/iter. Eval: 0.0007 s/iter. Total: 0.1556 s/iter. ETA=0:00:18\n",
      "\u001b[32m[09/06 14:44:02 d2.evaluation.evaluator]: \u001b[0mInference done 1114/1199. Dataloading: 0.0865 s/iter. Inference: 0.0681 s/iter. Eval: 0.0007 s/iter. Total: 0.1555 s/iter. ETA=0:00:13\n",
      "\u001b[32m[09/06 14:44:07 d2.evaluation.evaluator]: \u001b[0mInference done 1140/1199. Dataloading: 0.0873 s/iter. Inference: 0.0683 s/iter. Eval: 0.0007 s/iter. Total: 0.1564 s/iter. ETA=0:00:09\n",
      "\u001b[32m[09/06 14:44:12 d2.evaluation.evaluator]: \u001b[0mInference done 1170/1199. Dataloading: 0.0874 s/iter. Inference: 0.0685 s/iter. Eval: 0.0007 s/iter. Total: 0.1568 s/iter. ETA=0:00:04\n",
      "\u001b[32m[09/06 14:44:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:06.559928 (0.156248 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 14:44:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:21 (0.068426 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 14:44:16 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 14:44:16 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.49 seconds.\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.12 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.025\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.003 | 0.011  | 0.000  |  nan  |  nan  | 0.003 |\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|:-----------|:------|\n",
      "| angry      | 0.000 | sad        | 0.010 | surprised  | 0.000 |\n",
      "| happy      | 0.000 |            |       |            |       |\n",
      "\u001b[32m[09/06 14:44:17 d2.engine.defaults]: \u001b[0mEvaluation results for face_data_set_valid in csv format:\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[09/06 14:44:17 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0026,0.0114,0.0002,nan,nan,0.0031\n",
      "\u001b[32m[09/06 14:44:28 d2.engine.hooks]: \u001b[0mOverall training speed: 31 iterations in 0:01:25 (2.7558 s / it)\n",
      "\u001b[32m[09/06 14:44:28 d2.engine.hooks]: \u001b[0mTotal training time: 0:11:02 (0:09:37 on hooks)\n",
      "\u001b[32m[09/06 14:44:28 d2.utils.events]: \u001b[0m eta: 0:02:56  iter: 33  total_loss: 1.746  loss_cls: 1.508  loss_box_reg: 0.1241  loss_rpn_cls: 0.09474  loss_rpn_loc: 0.01029    time: 2.6681  last_time: 2.7676  data_time: 1.6060  last_data_time: 1.6695   lr: 6.8e-07  max_mem: 10128M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(cfg)\n\u001b[1;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39mresume_or_load(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mCustomTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Add evaluation after each epoch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_iter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/defaults.py:520\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/train_loop.py:155\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter, max_iter):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/defaults.py:530\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/train_loop.py:310\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mIf you want to do something with the losses, you can wrap the model.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    312\u001b[0m     losses \u001b[38;5;241m=\u001b[39m loss_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/meta_arch/rcnn.py:161\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    158\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/proposal_generator/rpn.py:452\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    images (ImageList): input images of length `N`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    loss: dict[Tensor] or None\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]\n\u001b[0;32m--> 452\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn_head(features)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/anchor_generator.py:230\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    features (list[Tensor]): list of backbone feature maps on which to generate anchors.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m        where Hi, Wi are resolution of the feature map divided by anchor stride.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m grid_sizes \u001b[38;5;241m=\u001b[39m [feature_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 230\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grid_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_sizes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pyre-ignore\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Boxes(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m anchors_over_all_feature_maps]\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/anchor_generator.py:174\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator._grid_anchors\u001b[0;34m(self, grid_sizes)\u001b[0m\n\u001b[1;32m    172\u001b[0m buffers: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_anchors\u001b[38;5;241m.\u001b[39mnamed_buffers()]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, stride, base_anchors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grid_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides, buffers):\n\u001b[0;32m--> 174\u001b[0m     shift_x, shift_y \u001b[38;5;241m=\u001b[39m \u001b[43m_create_grid_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_anchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     shifts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((shift_x, shift_y, shift_x, shift_y), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    177\u001b[0m     anchors\u001b[38;5;241m.\u001b[39mappend((shifts\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m+\u001b[39m base_anchors\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/anchor_generator.py:43\u001b[0m, in \u001b[0;36m_create_grid_offsets\u001b[0;34m(size, stride, offset, target_device_tensor)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_grid_offsets\u001b[39m(\n\u001b[1;32m     40\u001b[0m     size: List[\u001b[38;5;28mint\u001b[39m], stride: \u001b[38;5;28mint\u001b[39m, offset: \u001b[38;5;28mfloat\u001b[39m, target_device_tensor: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m     grid_height, grid_width \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m---> 43\u001b[0m     shifts_x \u001b[38;5;241m=\u001b[39m \u001b[43mmove_device_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_device_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     shifts_y \u001b[38;5;241m=\u001b[39m move_device_like(\n\u001b[1;32m     48\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(offset \u001b[38;5;241m*\u001b[39m stride, grid_height \u001b[38;5;241m*\u001b[39m stride, step\u001b[38;5;241m=\u001b[39mstride, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     49\u001b[0m         target_device_tensor,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     shift_y, shift_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(shifts_y, shifts_x)\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/jit/_trace.py:1441\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   1440\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> 1441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m     compiled_fn: Callable[P, R] \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/layers/wrappers.py:177\u001b[0m, in \u001b[0;36mmove_device_like\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_if_tracing\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_device_like\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, dst: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    Tracing friendly way to cast tensor to another tensor's device. Device will be treated\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    as constant during tracing, scripting the casting process as whole can workaround this issue.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_dir=None):\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_dir)\n",
    "\n",
    "    def train(self):\n",
    "        super().train()\n",
    "        # Add evaluation after each epoch\n",
    "        for epoch in range(self.start_iter, self.max_iter + 1):\n",
    "            if epoch % self.cfg.TEST.EVAL_PERIOD == 0:\n",
    "                self.do_test()\n",
    "            if epoch % self.cfg.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "                self.checkpointer.save(f\"model_{epoch}\")\n",
    "\n",
    "    def do_test(self):\n",
    "        # Perform evaluation\n",
    "        evaluator = self.build_evaluator(self.cfg, self.cfg.DATASETS.TEST[0], output_dir=self.cfg.OUTPUT_DIR)\n",
    "        val_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])\n",
    "        results = inference_on_dataset(self.model, val_loader, evaluator)\n",
    "        print(results)\n",
    "\n",
    "# Set up configuration as before\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"face_data_set\",)\n",
    "cfg.DATASETS.TEST = (\"face_data_set_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 100\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.INPUT.RANDOM_ROTATION = 30\n",
    "cfg.INPUT.CROP = CN({\"ENABLED\": True, \"TYPE\": \"relative_range\", \"SIZE\": [0.8, 0.8]})\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR = \"./models/faster_rcnn_R_50_FPN_3x\"\n",
    "cfg.TEST.EVAL_PERIOD = 10\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "\n",
    "setup_logger()\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 15:09:02 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[09/06 15:09:02 d2.data.datasets.coco]: \u001b[0mLoaded 5992 images in COCO format from data/ssd_rcnn_face/annotations.json\n",
      "\u001b[32m[09/06 15:09:02 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 5992 images left.\n",
      "\u001b[32m[09/06 15:09:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='relative_range', crop_size=[0.8, 0.8]), ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[09/06 15:09:02 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[09/06 15:09:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 15:09:02 d2.data.common]: \u001b[0mSerializing 5992 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 15:09:02 d2.data.common]: \u001b[0mSerialized dataset takes 1.79 MiB\n",
      "\u001b[32m[09/06 15:09:02 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=16\n",
      "\u001b[32m[09/06 15:09:02 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn4.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 15:09:02 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[09/06 15:09:15 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 15:09:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 15:09:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 15:09:15 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 15:09:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 15:09:15 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 15:09:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 15:09:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0409 s/iter. Inference: 0.0558 s/iter. Eval: 0.0007 s/iter. Total: 0.0974 s/iter. ETA=0:01:55\n",
      "\u001b[32m[09/06 15:09:22 d2.evaluation.evaluator]: \u001b[0mInference done 50/1199. Dataloading: 0.0676 s/iter. Inference: 0.0604 s/iter. Eval: 0.0007 s/iter. Total: 0.1289 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/06 15:09:28 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0853 s/iter. Inference: 0.0609 s/iter. Eval: 0.0007 s/iter. Total: 0.1471 s/iter. ETA=0:02:43\n",
      "\u001b[32m[09/06 15:09:33 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0876 s/iter. Inference: 0.0630 s/iter. Eval: 0.0007 s/iter. Total: 0.1514 s/iter. ETA=0:02:43\n",
      "\u001b[32m[09/06 15:09:38 d2.evaluation.evaluator]: \u001b[0mInference done 156/1199. Dataloading: 0.0839 s/iter. Inference: 0.0635 s/iter. Eval: 0.0007 s/iter. Total: 0.1482 s/iter. ETA=0:02:34\n",
      "\u001b[32m[09/06 15:09:43 d2.evaluation.evaluator]: \u001b[0mInference done 193/1199. Dataloading: 0.0830 s/iter. Inference: 0.0633 s/iter. Eval: 0.0007 s/iter. Total: 0.1472 s/iter. ETA=0:02:28\n",
      "\u001b[32m[09/06 15:09:49 d2.evaluation.evaluator]: \u001b[0mInference done 227/1199. Dataloading: 0.0842 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1491 s/iter. ETA=0:02:24\n",
      "\u001b[32m[09/06 15:09:54 d2.evaluation.evaluator]: \u001b[0mInference done 260/1199. Dataloading: 0.0846 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1495 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/06 15:10:00 d2.evaluation.evaluator]: \u001b[0mInference done 282/1199. Dataloading: 0.0936 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1585 s/iter. ETA=0:02:25\n",
      "\u001b[32m[09/06 15:10:05 d2.evaluation.evaluator]: \u001b[0mInference done 317/1199. Dataloading: 0.0921 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1570 s/iter. ETA=0:02:18\n",
      "\u001b[32m[09/06 15:10:10 d2.evaluation.evaluator]: \u001b[0mInference done 352/1199. Dataloading: 0.0906 s/iter. Inference: 0.0644 s/iter. Eval: 0.0007 s/iter. Total: 0.1558 s/iter. ETA=0:02:11\n",
      "\u001b[32m[09/06 15:10:15 d2.evaluation.evaluator]: \u001b[0mInference done 384/1199. Dataloading: 0.0916 s/iter. Inference: 0.0644 s/iter. Eval: 0.0007 s/iter. Total: 0.1567 s/iter. ETA=0:02:07\n",
      "\u001b[32m[09/06 15:10:20 d2.evaluation.evaluator]: \u001b[0mInference done 418/1199. Dataloading: 0.0908 s/iter. Inference: 0.0644 s/iter. Eval: 0.0007 s/iter. Total: 0.1560 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/06 15:10:25 d2.evaluation.evaluator]: \u001b[0mInference done 460/1199. Dataloading: 0.0879 s/iter. Inference: 0.0640 s/iter. Eval: 0.0007 s/iter. Total: 0.1526 s/iter. ETA=0:01:52\n",
      "\u001b[32m[09/06 15:10:30 d2.evaluation.evaluator]: \u001b[0mInference done 496/1199. Dataloading: 0.0872 s/iter. Inference: 0.0637 s/iter. Eval: 0.0007 s/iter. Total: 0.1517 s/iter. ETA=0:01:46\n",
      "\u001b[32m[09/06 15:10:35 d2.evaluation.evaluator]: \u001b[0mInference done 530/1199. Dataloading: 0.0868 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1516 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/06 15:10:40 d2.evaluation.evaluator]: \u001b[0mInference done 560/1199. Dataloading: 0.0869 s/iter. Inference: 0.0648 s/iter. Eval: 0.0007 s/iter. Total: 0.1525 s/iter. ETA=0:01:37\n",
      "\u001b[32m[09/06 15:10:45 d2.evaluation.evaluator]: \u001b[0mInference done 596/1199. Dataloading: 0.0860 s/iter. Inference: 0.0649 s/iter. Eval: 0.0007 s/iter. Total: 0.1517 s/iter. ETA=0:01:31\n",
      "\u001b[32m[09/06 15:10:50 d2.evaluation.evaluator]: \u001b[0mInference done 633/1199. Dataloading: 0.0856 s/iter. Inference: 0.0644 s/iter. Eval: 0.0007 s/iter. Total: 0.1508 s/iter. ETA=0:01:25\n",
      "\u001b[32m[09/06 15:10:56 d2.evaluation.evaluator]: \u001b[0mInference done 665/1199. Dataloading: 0.0863 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1512 s/iter. ETA=0:01:20\n",
      "\u001b[32m[09/06 15:11:01 d2.evaluation.evaluator]: \u001b[0mInference done 698/1199. Dataloading: 0.0867 s/iter. Inference: 0.0640 s/iter. Eval: 0.0007 s/iter. Total: 0.1516 s/iter. ETA=0:01:15\n",
      "\u001b[32m[09/06 15:11:06 d2.evaluation.evaluator]: \u001b[0mInference done 735/1199. Dataloading: 0.0861 s/iter. Inference: 0.0640 s/iter. Eval: 0.0007 s/iter. Total: 0.1509 s/iter. ETA=0:01:10\n",
      "\u001b[32m[09/06 15:11:11 d2.evaluation.evaluator]: \u001b[0mInference done 759/1199. Dataloading: 0.0881 s/iter. Inference: 0.0642 s/iter. Eval: 0.0007 s/iter. Total: 0.1531 s/iter. ETA=0:01:07\n",
      "\u001b[32m[09/06 15:11:16 d2.evaluation.evaluator]: \u001b[0mInference done 794/1199. Dataloading: 0.0879 s/iter. Inference: 0.0640 s/iter. Eval: 0.0007 s/iter. Total: 0.1527 s/iter. ETA=0:01:01\n",
      "\u001b[32m[09/06 15:11:21 d2.evaluation.evaluator]: \u001b[0mInference done 833/1199. Dataloading: 0.0870 s/iter. Inference: 0.0640 s/iter. Eval: 0.0007 s/iter. Total: 0.1518 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/06 15:11:43 d2.evaluation.evaluator]: \u001b[0mInference done 843/1199. Dataloading: 0.1109 s/iter. Inference: 0.0641 s/iter. Eval: 0.0007 s/iter. Total: 0.1759 s/iter. ETA=0:01:02\n",
      "\u001b[32m[09/06 15:11:48 d2.evaluation.evaluator]: \u001b[0mInference done 879/1199. Dataloading: 0.1097 s/iter. Inference: 0.0640 s/iter. Eval: 0.0007 s/iter. Total: 0.1745 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/06 15:11:53 d2.evaluation.evaluator]: \u001b[0mInference done 913/1199. Dataloading: 0.1089 s/iter. Inference: 0.0639 s/iter. Eval: 0.0007 s/iter. Total: 0.1736 s/iter. ETA=0:00:49\n",
      "\u001b[32m[09/06 15:11:58 d2.evaluation.evaluator]: \u001b[0mInference done 955/1199. Dataloading: 0.1068 s/iter. Inference: 0.0638 s/iter. Eval: 0.0007 s/iter. Total: 0.1713 s/iter. ETA=0:00:41\n",
      "\u001b[32m[09/06 15:12:03 d2.evaluation.evaluator]: \u001b[0mInference done 983/1199. Dataloading: 0.1070 s/iter. Inference: 0.0637 s/iter. Eval: 0.0007 s/iter. Total: 0.1715 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/06 15:12:09 d2.evaluation.evaluator]: \u001b[0mInference done 1019/1199. Dataloading: 0.1068 s/iter. Inference: 0.0637 s/iter. Eval: 0.0007 s/iter. Total: 0.1712 s/iter. ETA=0:00:30\n",
      "\u001b[32m[09/06 15:12:14 d2.evaluation.evaluator]: \u001b[0mInference done 1056/1199. Dataloading: 0.1056 s/iter. Inference: 0.0636 s/iter. Eval: 0.0007 s/iter. Total: 0.1700 s/iter. ETA=0:00:24\n",
      "\u001b[32m[09/06 15:12:20 d2.evaluation.evaluator]: \u001b[0mInference done 1089/1199. Dataloading: 0.1053 s/iter. Inference: 0.0636 s/iter. Eval: 0.0007 s/iter. Total: 0.1696 s/iter. ETA=0:00:18\n",
      "\u001b[32m[09/06 15:12:25 d2.evaluation.evaluator]: \u001b[0mInference done 1126/1199. Dataloading: 0.1050 s/iter. Inference: 0.0634 s/iter. Eval: 0.0007 s/iter. Total: 0.1692 s/iter. ETA=0:00:12\n",
      "\u001b[32m[09/06 15:12:31 d2.evaluation.evaluator]: \u001b[0mInference done 1161/1199. Dataloading: 0.1044 s/iter. Inference: 0.0634 s/iter. Eval: 0.0007 s/iter. Total: 0.1685 s/iter. ETA=0:00:06\n",
      "\u001b[32m[09/06 15:12:36 d2.evaluation.evaluator]: \u001b[0mInference done 1198/1199. Dataloading: 0.1035 s/iter. Inference: 0.0634 s/iter. Eval: 0.0007 s/iter. Total: 0.1676 s/iter. ETA=0:00:00\n",
      "\u001b[32m[09/06 15:12:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:20.040713 (0.167538 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 15:12:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:15 (0.063375 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 15:12:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 15:12:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.43s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 15:12:36 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.51 seconds.\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.12 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.008\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.001 | 0.006  | 0.000  |  nan  |  nan  | 0.001 |\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|:-----------|:------|\n",
      "| angry      | 0.000 | sad        | 0.000 | surprised  | 0.000 |\n",
      "| happy      | 0.003 |            |       |            |       |\n",
      "\u001b[32m[09/06 15:12:37 d2.engine.defaults]: \u001b[0mEvaluation results for face_data_set_valid in csv format:\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[09/06 15:12:37 d2.evaluation.testing]: \u001b[0mcopypaste: 0.0008,0.0056,0.0000,nan,nan,0.0008\n",
      "\u001b[32m[09/06 15:12:50 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 15:12:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 15:12:50 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 15:12:50 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 15:12:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 15:12:50 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 15:12:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 15:12:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0499 s/iter. Inference: 0.0518 s/iter. Eval: 0.0006 s/iter. Total: 0.1023 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/06 15:12:57 d2.evaluation.evaluator]: \u001b[0mInference done 50/1199. Dataloading: 0.0678 s/iter. Inference: 0.0592 s/iter. Eval: 0.0007 s/iter. Total: 0.1277 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/06 15:13:03 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.0866 s/iter. Inference: 0.0612 s/iter. Eval: 0.0007 s/iter. Total: 0.1486 s/iter. ETA=0:02:45\n",
      "\u001b[32m[09/06 15:13:09 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.0901 s/iter. Inference: 0.0654 s/iter. Eval: 0.0008 s/iter. Total: 0.1564 s/iter. ETA=0:02:48\n",
      "\u001b[32m[09/06 15:13:14 d2.evaluation.evaluator]: \u001b[0mInference done 157/1199. Dataloading: 0.0839 s/iter. Inference: 0.0658 s/iter. Eval: 0.0008 s/iter. Total: 0.1505 s/iter. ETA=0:02:36\n",
      "\u001b[32m[09/06 15:13:20 d2.evaluation.evaluator]: \u001b[0mInference done 193/1199. Dataloading: 0.0832 s/iter. Inference: 0.0660 s/iter. Eval: 0.0008 s/iter. Total: 0.1500 s/iter. ETA=0:02:30\n",
      "\u001b[32m[09/06 15:13:25 d2.evaluation.evaluator]: \u001b[0mInference done 227/1199. Dataloading: 0.0841 s/iter. Inference: 0.0654 s/iter. Eval: 0.0008 s/iter. Total: 0.1503 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/06 15:13:30 d2.evaluation.evaluator]: \u001b[0mInference done 257/1199. Dataloading: 0.0857 s/iter. Inference: 0.0659 s/iter. Eval: 0.0007 s/iter. Total: 0.1524 s/iter. ETA=0:02:23\n",
      "\u001b[32m[09/06 15:13:36 d2.evaluation.evaluator]: \u001b[0mInference done 282/1199. Dataloading: 0.0937 s/iter. Inference: 0.0662 s/iter. Eval: 0.0007 s/iter. Total: 0.1608 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/06 15:13:41 d2.evaluation.evaluator]: \u001b[0mInference done 314/1199. Dataloading: 0.0926 s/iter. Inference: 0.0670 s/iter. Eval: 0.0007 s/iter. Total: 0.1605 s/iter. ETA=0:02:22\n",
      "\u001b[32m[09/06 15:13:46 d2.evaluation.evaluator]: \u001b[0mInference done 348/1199. Dataloading: 0.0920 s/iter. Inference: 0.0671 s/iter. Eval: 0.0008 s/iter. Total: 0.1600 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/06 15:13:52 d2.evaluation.evaluator]: \u001b[0mInference done 381/1199. Dataloading: 0.0920 s/iter. Inference: 0.0669 s/iter. Eval: 0.0007 s/iter. Total: 0.1598 s/iter. ETA=0:02:10\n",
      "\u001b[32m[09/06 15:13:56 d2.engine.hooks]: \u001b[0mOverall training speed: 7 iterations in 0:00:20 (2.9155 s / it)\n",
      "\u001b[32m[09/06 15:13:56 d2.engine.hooks]: \u001b[0mTotal training time: 0:04:48 (0:04:28 on hooks)\n",
      "\u001b[32m[09/06 15:13:56 d2.utils.events]: \u001b[0m eta: 0:03:44  iter: 9  total_loss: 1.876  loss_cls: 1.625  loss_box_reg: 0.1321  loss_rpn_cls: 0.09249  loss_rpn_loc: 0.01025    time: 2.5510  last_time: 2.3832  data_time: 1.5662  last_data_time: 1.4313   lr: 9.1e-07  max_mem: 4692M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(cfg)\n\u001b[1;32m     76\u001b[0m trainer\u001b[38;5;241m.\u001b[39mresume_or_load(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mCustomTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Add evaluation after each epoch\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mSOLVER\u001b[38;5;241m.\u001b[39mMAX_ITER \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEVAL_PERIOD):\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/defaults.py:520\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/train_loop.py:156\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_step()\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/train_loop.py:190\u001b[0m, in \u001b[0;36mTrainerBase.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[0;32m--> 190\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/hooks.py:556\u001b[0m, in \u001b[0;36mEvalHook.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_period \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m next_iter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# do the last eval in after_train\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_iter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_iter:\n\u001b[0;32m--> 556\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/hooks.py:529\u001b[0m, in \u001b[0;36mEvalHook._do_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    533\u001b[0m             results, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    534\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval function must return a dict. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(results)\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/defaults.py:489\u001b[0m, in \u001b[0;36mDefaultTrainer.build_hooks.<locals>.test_and_save_results\u001b[0;34m()\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_and_save_results\u001b[39m():\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_eval_results\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/engine/defaults.py:653\u001b[0m, in \u001b[0;36mDefaultTrainer.test\u001b[0;34m(cls, cfg, model, evaluators)\u001b[0m\n\u001b[1;32m    651\u001b[0m         results[dataset_name] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m results_i \u001b[38;5;241m=\u001b[39m \u001b[43minference_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m results[dataset_name] \u001b[38;5;241m=\u001b[39m results_i\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/evaluation/evaluator.py:165\u001b[0m, in \u001b[0;36minference_on_dataset\u001b[0;34m(model, data_loader, evaluator, callbacks)\u001b[0m\n\u001b[1;32m    163\u001b[0m start_compute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mget(callbacks \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[0;32m--> 165\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mget(callbacks \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/p3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/meta_arch/rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/meta_arch/rcnn.py:220\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_postprocess:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScripting is not supported for postprocess.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneralizedRCNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/meta_arch/rcnn.py:248\u001b[0m, in \u001b[0;36mGeneralizedRCNN._postprocess\u001b[0;34m(instances, batched_inputs, image_sizes)\u001b[0m\n\u001b[1;32m    246\u001b[0m     height \u001b[38;5;241m=\u001b[39m input_per_image\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_size[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    247\u001b[0m     width \u001b[38;5;241m=\u001b[39m input_per_image\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_size[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 248\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mdetector_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_per_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     processed_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: r})\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processed_results\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/modeling/postprocessing.py:58\u001b[0m, in \u001b[0;36mdetector_postprocess\u001b[0;34m(results, output_height, output_width, mask_threshold)\u001b[0m\n\u001b[1;32m     55\u001b[0m output_boxes\u001b[38;5;241m.\u001b[39mscale(scale_x, scale_y)\n\u001b[1;32m     56\u001b[0m output_boxes\u001b[38;5;241m.\u001b[39mclip(results\u001b[38;5;241m.\u001b[39mimage_size)\n\u001b[0;32m---> 58\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_boxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mhas(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_masks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results\u001b[38;5;241m.\u001b[39mpred_masks, ROIMasks):\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/structures/instances.py:141\u001b[0m, in \u001b[0;36mInstances.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    139\u001b[0m ret \u001b[38;5;241m=\u001b[39m Instances(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_size)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fields\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/mnt/e/py_data/project_3_git/detectron2/detectron2/structures/instances.py:69\u001b[0m, in \u001b[0;36mInstances.set\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find field \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in the given Instances!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fields[name]\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Set the field named `name` to `value`.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    The length of `value` must be the number of instances,\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    and must agree with other existing fields in this object.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import CfgNode as CN\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_dir=None):\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_dir)\n",
    "    \n",
    "    def train(self):\n",
    "        self.results = {\"train\": [], \"test\": []}\n",
    "        super().train()\n",
    "        \n",
    "        # Add evaluation after each epoch\n",
    "        for epoch in range(0, self.cfg.SOLVER.MAX_ITER + 1, self.cfg.TEST.EVAL_PERIOD):\n",
    "            self.do_test(epoch)\n",
    "            if epoch % self.cfg.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "                self.checkpointer.save(f\"model_{epoch}\")\n",
    "\n",
    "    def do_test(self, epoch):\n",
    "        try:\n",
    "            # Perform evaluation\n",
    "            evaluator = self.build_evaluator(self.cfg, self.cfg.DATASETS.TEST[0], output_dir=self.cfg.OUTPUT_DIR)\n",
    "            val_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])\n",
    "            results = inference_on_dataset(self.model, val_loader, evaluator)\n",
    "            self.results[\"test\"].append({\"epoch\": epoch, \"results\": results})\n",
    "            print(\"Evaluation results:\", results)\n",
    "    \n",
    "            # Save the results as JSON\n",
    "            self.save_results_as_json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation at epoch {epoch}: {e}\")\n",
    "    \n",
    "    def save_results_as_json(self):\n",
    "        try:\n",
    "            output_file_path = os.path.join(self.cfg.OUTPUT_DIR, \"train_test_results.json\")\n",
    "            print(f\"Saving results to: {output_file_path}\")\n",
    "            with open(output_file_path, 'w') as f:\n",
    "                json.dump(self.results, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results as JSON: {e}\")\n",
    "\n",
    "\n",
    "# Set up configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"face_data_set\",)\n",
    "cfg.DATASETS.TEST = (\"face_data_set_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 100\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.INPUT.RANDOM_ROTATION = 30\n",
    "cfg.INPUT.CROP = CN({\"ENABLED\": True, \"TYPE\": \"relative_range\", \"SIZE\": [0.8, 0.8]})\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR = \"./models/faster_rcnn_R_50_FPN_3x\"\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "\n",
    "setup_logger()\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
