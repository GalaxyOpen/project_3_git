{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /mnt/e/py_data/project_3_git\n"
     ]
    }
   ],
   "source": [
    "# 기본 작업 경로 설정\n",
    "\n",
    "import os\n",
    "notebook_path = os.path.abspath(\"../readme.md\")\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# 현재 작업 디렉토리 출력\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# 데이터셋 경로와 형식 설정\n",
    "register_coco_instances(\"face_data_set\", {}, \"data/ssd_rcnn_face/annotations.json\", \"data/yolo_data/train/\")\n",
    "register_coco_instances(\"face_data_set_valid\", {}, \"data/ssd_rcnn_face/annotations_val.json\", \"data/yolo_data/val/\")\n",
    "\n",
    "# 메타데이터를 확인.\n",
    "metadata = MetadataCatalog.get(\"face_data_set\")\n",
    "dataset_dicts = DatasetCatalog.get(\"face_data_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 15:25:53 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[09/06 15:25:54 d2.data.datasets.coco]: \u001b[0mLoaded 5992 images in COCO format from data/ssd_rcnn_face/annotations.json\n",
      "\u001b[32m[09/06 15:25:54 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 5992 images left.\n",
      "\u001b[32m[09/06 15:25:54 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   angry    | 1500         |    sad     | 1499         | surprised  | 1499         |\n",
      "|   happy    | 1494         |            |              |            |              |\n",
      "|   total    | 5992         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[09/06 15:25:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='relative_range', crop_size=[0.8, 0.8]), ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[09/06 15:25:54 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[09/06 15:25:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 15:25:54 d2.data.common]: \u001b[0mSerializing 5992 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 15:25:54 d2.data.common]: \u001b[0mSerialized dataset takes 1.79 MiB\n",
      "\u001b[32m[09/06 15:25:54 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=16\n",
      "\u001b[32m[09/06 15:25:54 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn3.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn4.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 15:25:54 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/p3/lib/python3.11/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 15:26:45 d2.utils.events]: \u001b[0m eta: 0:39:36  iter: 19  total_loss: 2.064  loss_cls: 1.827  loss_box_reg: 0.1404  loss_rpn_cls: 0.08922  loss_rpn_loc: 0.009855    time: 2.4953  last_time: 2.2672  data_time: 1.5805  last_data_time: 1.4263   lr: 1.9962e-05  max_mem: 3888M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 15:26:45.984192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-06 15:26:46.000402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-06 15:26:46.006009: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-06 15:26:46.019024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-06 15:26:47.181203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[09/06 15:27:35 d2.utils.events]: \u001b[0m eta: 0:37:49  iter: 39  total_loss: 1.594  loss_cls: 1.298  loss_box_reg: 0.1646  loss_rpn_cls: 0.08414  loss_rpn_loc: 0.009699    time: 2.4312  last_time: 2.6242  data_time: 1.4421  last_data_time: 1.7176   lr: 3.9922e-05  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:28:24 d2.utils.events]: \u001b[0m eta: 0:37:02  iter: 59  total_loss: 0.95  loss_cls: 0.6248  loss_box_reg: 0.2442  loss_rpn_cls: 0.06401  loss_rpn_loc: 0.01028    time: 2.4315  last_time: 1.9801  data_time: 1.5015  last_data_time: 1.1361   lr: 5.9882e-05  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:29:19 d2.utils.events]: \u001b[0m eta: 0:36:54  iter: 79  total_loss: 0.8377  loss_cls: 0.4112  loss_box_reg: 0.379  loss_rpn_cls: 0.03944  loss_rpn_loc: 0.009045    time: 2.5121  last_time: 3.1003  data_time: 1.7906  last_data_time: 2.1011   lr: 7.9842e-05  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:30:11 d2.utils.events]: \u001b[0m eta: 0:36:18  iter: 99  total_loss: 0.7889  loss_cls: 0.3553  loss_box_reg: 0.4028  loss_rpn_cls: 0.02307  loss_rpn_loc: 0.009188    time: 2.5314  last_time: 2.6414  data_time: 1.6278  last_data_time: 1.6545   lr: 9.9802e-05  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:31:03 d2.utils.events]: \u001b[0m eta: 0:35:44  iter: 119  total_loss: 0.7663  loss_cls: 0.3248  loss_box_reg: 0.4244  loss_rpn_cls: 0.01596  loss_rpn_loc: 0.007649    time: 2.5454  last_time: 2.5951  data_time: 1.6411  last_data_time: 1.5585   lr: 0.00011976  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:31:53 d2.utils.events]: \u001b[0m eta: 0:35:00  iter: 139  total_loss: 0.772  loss_cls: 0.3027  loss_box_reg: 0.4449  loss_rpn_cls: 0.01486  loss_rpn_loc: 0.00833    time: 2.5384  last_time: 2.5263  data_time: 1.5119  last_data_time: 1.6191   lr: 0.00013972  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:32:42 d2.utils.events]: \u001b[0m eta: 0:34:23  iter: 159  total_loss: 0.7459  loss_cls: 0.2847  loss_box_reg: 0.4412  loss_rpn_cls: 0.01234  loss_rpn_loc: 0.007562    time: 2.5275  last_time: 2.7396  data_time: 1.4843  last_data_time: 1.6895   lr: 0.00015968  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:33:33 d2.utils.events]: \u001b[0m eta: 0:33:46  iter: 179  total_loss: 0.7448  loss_cls: 0.2769  loss_box_reg: 0.4484  loss_rpn_cls: 0.01165  loss_rpn_loc: 0.007054    time: 2.5291  last_time: 2.7795  data_time: 1.5598  last_data_time: 1.8454   lr: 0.00017964  max_mem: 3888M\n",
      "\u001b[32m[09/06 15:34:24 d2.utils.events]: \u001b[0m eta: 0:32:57  iter: 199  total_loss: 0.7512  loss_cls: 0.2725  loss_box_reg: 0.4671  loss_rpn_cls: 0.007114  loss_rpn_loc: 0.007054    time: 2.5305  last_time: 2.4726  data_time: 1.5365  last_data_time: 1.4970   lr: 0.0001996  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:35:13 d2.utils.events]: \u001b[0m eta: 0:32:09  iter: 219  total_loss: 0.7835  loss_cls: 0.2791  loss_box_reg: 0.4925  loss_rpn_cls: 0.006572  loss_rpn_loc: 0.006125    time: 2.5252  last_time: 2.2842  data_time: 1.5172  last_data_time: 1.4480   lr: 0.00021956  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:36:06 d2.utils.events]: \u001b[0m eta: 0:31:20  iter: 239  total_loss: 0.7706  loss_cls: 0.2725  loss_box_reg: 0.4902  loss_rpn_cls: 0.007006  loss_rpn_loc: 0.006207    time: 2.5354  last_time: 2.0453  data_time: 1.6769  last_data_time: 1.1694   lr: 0.00023952  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:36:56 d2.utils.events]: \u001b[0m eta: 0:30:26  iter: 259  total_loss: 0.8122  loss_cls: 0.2805  loss_box_reg: 0.5116  loss_rpn_cls: 0.006782  loss_rpn_loc: 0.005938    time: 2.5319  last_time: 2.4638  data_time: 1.5594  last_data_time: 1.5461   lr: 0.00025948  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:37:46 d2.utils.events]: \u001b[0m eta: 0:29:35  iter: 279  total_loss: 0.791  loss_cls: 0.2827  loss_box_reg: 0.4962  loss_rpn_cls: 0.00462  loss_rpn_loc: 0.005709    time: 2.5298  last_time: 3.5040  data_time: 1.5543  last_data_time: 2.4818   lr: 0.00027944  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:38:36 d2.utils.events]: \u001b[0m eta: 0:28:40  iter: 299  total_loss: 0.8583  loss_cls: 0.3063  loss_box_reg: 0.5415  loss_rpn_cls: 0.004488  loss_rpn_loc: 0.005432    time: 2.5280  last_time: 2.6418  data_time: 1.5520  last_data_time: 1.7260   lr: 0.0002994  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:39:24 d2.utils.events]: \u001b[0m eta: 0:27:41  iter: 319  total_loss: 0.8169  loss_cls: 0.3115  loss_box_reg: 0.4935  loss_rpn_cls: 0.003706  loss_rpn_loc: 0.005039    time: 2.5178  last_time: 2.3044  data_time: 1.4521  last_data_time: 1.4496   lr: 0.00031936  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:40:13 d2.utils.events]: \u001b[0m eta: 0:26:54  iter: 339  total_loss: 0.777  loss_cls: 0.3439  loss_box_reg: 0.4302  loss_rpn_cls: 0.003824  loss_rpn_loc: 0.005306    time: 2.5160  last_time: 2.4239  data_time: 1.4966  last_data_time: 1.4586   lr: 0.00033932  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:41:06 d2.utils.events]: \u001b[0m eta: 0:26:06  iter: 359  total_loss: 0.6715  loss_cls: 0.3501  loss_box_reg: 0.3142  loss_rpn_cls: 0.005106  loss_rpn_loc: 0.005634    time: 2.5226  last_time: 2.3279  data_time: 1.6221  last_data_time: 1.3966   lr: 0.00035928  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:42:00 d2.utils.events]: \u001b[0m eta: 0:25:20  iter: 379  total_loss: 0.6214  loss_cls: 0.3434  loss_box_reg: 0.2693  loss_rpn_cls: 0.004946  loss_rpn_loc: 0.004535    time: 2.5318  last_time: 2.5929  data_time: 1.7115  last_data_time: 1.6222   lr: 0.00037924  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:42:47 d2.utils.events]: \u001b[0m eta: 0:24:27  iter: 399  total_loss: 0.5896  loss_cls: 0.3314  loss_box_reg: 0.2462  loss_rpn_cls: 0.002582  loss_rpn_loc: 0.004043    time: 2.5234  last_time: 2.0761  data_time: 1.4327  last_data_time: 1.2500   lr: 0.0003992  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:43:37 d2.utils.events]: \u001b[0m eta: 0:23:37  iter: 419  total_loss: 0.5117  loss_cls: 0.3056  loss_box_reg: 0.2041  loss_rpn_cls: 0.002936  loss_rpn_loc: 0.004129    time: 2.5216  last_time: 2.4644  data_time: 1.5438  last_data_time: 1.5668   lr: 0.00041916  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:44:27 d2.utils.events]: \u001b[0m eta: 0:22:49  iter: 439  total_loss: 0.51  loss_cls: 0.3184  loss_box_reg: 0.1905  loss_rpn_cls: 0.002188  loss_rpn_loc: 0.003464    time: 2.5195  last_time: 2.5536  data_time: 1.5280  last_data_time: 1.6403   lr: 0.00043912  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:45:12 d2.utils.events]: \u001b[0m eta: 0:21:55  iter: 459  total_loss: 0.5174  loss_cls: 0.3133  loss_box_reg: 0.1955  loss_rpn_cls: 0.001569  loss_rpn_loc: 0.003214    time: 2.5094  last_time: 2.0828  data_time: 1.3822  last_data_time: 1.1595   lr: 0.00045908  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:46:03 d2.utils.events]: \u001b[0m eta: 0:21:06  iter: 479  total_loss: 0.4836  loss_cls: 0.2948  loss_box_reg: 0.175  loss_rpn_cls: 0.001785  loss_rpn_loc: 0.003039    time: 2.5099  last_time: 3.2104  data_time: 1.5825  last_data_time: 2.2257   lr: 0.00047904  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:46:53 d2.utils.events]: \u001b[0m eta: 0:20:14  iter: 499  total_loss: 0.4899  loss_cls: 0.3042  loss_box_reg: 0.1828  loss_rpn_cls: 0.001136  loss_rpn_loc: 0.003263    time: 2.5054  last_time: 2.3915  data_time: 1.4848  last_data_time: 1.4302   lr: 0.000499  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:47:45 d2.utils.events]: \u001b[0m eta: 0:19:28  iter: 519  total_loss: 0.4913  loss_cls: 0.308  loss_box_reg: 0.1756  loss_rpn_cls: 0.0009043  loss_rpn_loc: 0.002913    time: 2.5095  last_time: 2.2053  data_time: 1.6323  last_data_time: 1.2872   lr: 0.00047017  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:48:36 d2.utils.events]: \u001b[0m eta: 0:18:40  iter: 539  total_loss: 0.4823  loss_cls: 0.3083  loss_box_reg: 0.1721  loss_rpn_cls: 0.001031  loss_rpn_loc: 0.002692    time: 2.5099  last_time: 2.2791  data_time: 1.5643  last_data_time: 1.3133   lr: 0.00043889  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:49:25 d2.utils.events]: \u001b[0m eta: 0:17:52  iter: 559  total_loss: 0.4925  loss_cls: 0.3191  loss_box_reg: 0.1758  loss_rpn_cls: 0.001065  loss_rpn_loc: 0.002533    time: 2.5084  last_time: 2.0842  data_time: 1.4788  last_data_time: 1.1586   lr: 0.00040785  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:50:23 d2.utils.events]: \u001b[0m eta: 0:17:06  iter: 579  total_loss: 0.4882  loss_cls: 0.3155  loss_box_reg: 0.1694  loss_rpn_cls: 0.001194  loss_rpn_loc: 0.002563    time: 2.5215  last_time: 2.8268  data_time: 1.8270  last_data_time: 1.7499   lr: 0.00037718  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:51:24 d2.utils.events]: \u001b[0m eta: 0:16:20  iter: 599  total_loss: 0.4831  loss_cls: 0.3221  loss_box_reg: 0.1667  loss_rpn_cls: 0.001364  loss_rpn_loc: 0.002455    time: 2.5385  last_time: 2.6401  data_time: 1.9627  last_data_time: 1.6820   lr: 0.00034699  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:52:23 d2.utils.events]: \u001b[0m eta: 0:15:36  iter: 619  total_loss: 0.4941  loss_cls: 0.3123  loss_box_reg: 0.1756  loss_rpn_cls: 0.000835  loss_rpn_loc: 0.002399    time: 2.5524  last_time: 4.3586  data_time: 1.8969  last_data_time: 3.1807   lr: 0.0003174  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:53:19 d2.utils.events]: \u001b[0m eta: 0:14:48  iter: 639  total_loss: 0.4853  loss_cls: 0.3127  loss_box_reg: 0.1703  loss_rpn_cls: 0.0004774  loss_rpn_loc: 0.002366    time: 2.5599  last_time: 3.3728  data_time: 1.7469  last_data_time: 2.1904   lr: 0.00028853  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:54:19 d2.utils.events]: \u001b[0m eta: 0:14:02  iter: 659  total_loss: 0.4814  loss_cls: 0.3127  loss_box_reg: 0.166  loss_rpn_cls: 0.0005603  loss_rpn_loc: 0.002263    time: 2.5731  last_time: 2.6413  data_time: 1.8661  last_data_time: 1.5840   lr: 0.0002605  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:55:15 d2.utils.events]: \u001b[0m eta: 0:13:15  iter: 679  total_loss: 0.4759  loss_cls: 0.31  loss_box_reg: 0.157  loss_rpn_cls: 0.0002732  loss_rpn_loc: 0.002443    time: 2.5804  last_time: 2.5400  data_time: 1.7360  last_data_time: 1.4828   lr: 0.00023341  max_mem: 4118M\n",
      "\u001b[32m[09/06 15:56:15 d2.utils.events]: \u001b[0m eta: 0:12:28  iter: 699  total_loss: 0.4675  loss_cls: 0.3095  loss_box_reg: 0.1526  loss_rpn_cls: 0.0003603  loss_rpn_loc: 0.002499    time: 2.5920  last_time: 3.0530  data_time: 1.8217  last_data_time: 1.8089   lr: 0.00020738  max_mem: 4119M\n",
      "\u001b[32m[09/06 15:57:22 d2.utils.events]: \u001b[0m eta: 0:11:40  iter: 719  total_loss: 0.4769  loss_cls: 0.3124  loss_box_reg: 0.1598  loss_rpn_cls: 0.0002284  loss_rpn_loc: 0.002209    time: 2.6137  last_time: 4.0572  data_time: 2.1887  last_data_time: 2.8278   lr: 0.0001825  max_mem: 4119M\n",
      "\u001b[32m[09/06 15:58:34 d2.utils.events]: \u001b[0m eta: 0:10:53  iter: 739  total_loss: 0.4587  loss_cls: 0.3115  loss_box_reg: 0.1475  loss_rpn_cls: 0.0008393  loss_rpn_loc: 0.002372    time: 2.6399  last_time: 2.6888  data_time: 2.3254  last_data_time: 1.6683   lr: 0.00015887  max_mem: 4119M\n",
      "\u001b[32m[09/06 15:59:36 d2.utils.events]: \u001b[0m eta: 0:10:05  iter: 759  total_loss: 0.4587  loss_cls: 0.3072  loss_box_reg: 0.1527  loss_rpn_cls: 0.0007782  loss_rpn_loc: 0.002391    time: 2.6520  last_time: 3.1376  data_time: 1.9308  last_data_time: 1.9699   lr: 0.00013659  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:00:44 d2.utils.events]: \u001b[0m eta: 0:09:17  iter: 779  total_loss: 0.4533  loss_cls: 0.2965  loss_box_reg: 0.1526  loss_rpn_cls: 0.0005674  loss_rpn_loc: 0.002571    time: 2.6716  last_time: 3.2717  data_time: 2.1868  last_data_time: 1.9882   lr: 0.00011575  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:01:49 d2.utils.events]: \u001b[0m eta: 0:08:28  iter: 799  total_loss: 0.4664  loss_cls: 0.3087  loss_box_reg: 0.153  loss_rpn_cls: 0.0002521  loss_rpn_loc: 0.002167    time: 2.6861  last_time: 3.0018  data_time: 2.0560  last_data_time: 1.8585   lr: 9.6417e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:02:46 d2.utils.events]: \u001b[0m eta: 0:07:38  iter: 819  total_loss: 0.4592  loss_cls: 0.3029  loss_box_reg: 0.1593  loss_rpn_cls: 0.0006536  loss_rpn_loc: 0.002451    time: 2.6897  last_time: 2.7307  data_time: 1.7718  last_data_time: 1.6603   lr: 7.868e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:03:45 d2.utils.events]: \u001b[0m eta: 0:06:49  iter: 839  total_loss: 0.4512  loss_cls: 0.3029  loss_box_reg: 0.146  loss_rpn_cls: 0.0006067  loss_rpn_loc: 0.002046    time: 2.6956  last_time: 3.3121  data_time: 1.8541  last_data_time: 2.1693   lr: 6.2606e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:04:45 d2.utils.events]: \u001b[0m eta: 0:05:59  iter: 859  total_loss: 0.4611  loss_cls: 0.3043  loss_box_reg: 0.1508  loss_rpn_cls: 0.0003413  loss_rpn_loc: 0.002285    time: 2.7028  last_time: 2.6280  data_time: 1.8902  last_data_time: 1.6334   lr: 4.8258e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:05:40 d2.utils.events]: \u001b[0m eta: 0:05:08  iter: 879  total_loss: 0.4494  loss_cls: 0.3007  loss_box_reg: 0.1461  loss_rpn_cls: 0.000658  loss_rpn_loc: 0.002252    time: 2.7039  last_time: 2.1406  data_time: 1.7213  last_data_time: 1.2085   lr: 3.5692e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:06:36 d2.utils.events]: \u001b[0m eta: 0:04:17  iter: 899  total_loss: 0.4478  loss_cls: 0.2935  loss_box_reg: 0.146  loss_rpn_cls: 0.0003898  loss_rpn_loc: 0.002267    time: 2.7065  last_time: 2.8832  data_time: 1.7719  last_data_time: 1.7929   lr: 2.4959e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:07:36 d2.utils.events]: \u001b[0m eta: 0:03:26  iter: 919  total_loss: 0.4304  loss_cls: 0.2855  loss_box_reg: 0.1417  loss_rpn_cls: 0.0002337  loss_rpn_loc: 0.00231    time: 2.7122  last_time: 3.0387  data_time: 1.9036  last_data_time: 1.9447   lr: 1.6101e-05  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:08:29 d2.utils.events]: \u001b[0m eta: 0:02:34  iter: 939  total_loss: 0.4553  loss_cls: 0.3018  loss_box_reg: 0.1492  loss_rpn_cls: 0.0008257  loss_rpn_loc: 0.002382    time: 2.7116  last_time: 3.2710  data_time: 1.6334  last_data_time: 2.1975   lr: 9.1531e-06  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:09:29 d2.utils.events]: \u001b[0m eta: 0:01:43  iter: 959  total_loss: 0.4533  loss_cls: 0.2992  loss_box_reg: 0.1479  loss_rpn_cls: 0.0002747  loss_rpn_loc: 0.002416    time: 2.7168  last_time: 2.6905  data_time: 1.8501  last_data_time: 1.6529   lr: 4.142e-06  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:10:27 d2.utils.events]: \u001b[0m eta: 0:00:51  iter: 979  total_loss: 0.4403  loss_cls: 0.2955  loss_box_reg: 0.1474  loss_rpn_cls: 0.0003942  loss_rpn_loc: 0.002336    time: 2.7212  last_time: 2.5324  data_time: 1.8552  last_data_time: 1.5894   lr: 1.0877e-06  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:11:32 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 999  total_loss: 0.4518  loss_cls: 0.2991  loss_box_reg: 0.1567  loss_rpn_cls: 0.0003585  loss_rpn_loc: 0.002246    time: 2.7263  last_time: 3.1676  data_time: 1.8778  last_data_time: 2.0618   lr: 2.4674e-09  max_mem: 4119M\n",
      "\u001b[32m[09/06 16:11:32 d2.engine.hooks]: \u001b[0mOverall training speed: 998 iterations in 0:45:20 (2.7263 s / it)\n",
      "\u001b[32m[09/06 16:11:32 d2.engine.hooks]: \u001b[0mTotal training time: 0:45:32 (0:00:11 on hooks)\n",
      "\u001b[32m[09/06 16:11:32 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 16:11:33 d2.data.build]: \u001b[0mDistribution of instances among all 4 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   angry    | 300          |    sad     | 300          | surprised  | 300          |\n",
      "|   happy    | 299          |            |              |            |              |\n",
      "|   total    | 1199         |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[09/06 16:11:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 16:11:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 16:11:33 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 16:11:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 16:11:33 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 16:11:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 16:11:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0608 s/iter. Inference: 0.1090 s/iter. Eval: 0.0006 s/iter. Total: 0.1703 s/iter. ETA=0:03:22\n",
      "\u001b[32m[09/06 16:11:40 d2.evaluation.evaluator]: \u001b[0mInference done 43/1199. Dataloading: 0.0770 s/iter. Inference: 0.0808 s/iter. Eval: 0.0007 s/iter. Total: 0.1586 s/iter. ETA=0:03:03\n",
      "\u001b[32m[09/06 16:11:45 d2.evaluation.evaluator]: \u001b[0mInference done 68/1199. Dataloading: 0.0905 s/iter. Inference: 0.0846 s/iter. Eval: 0.0007 s/iter. Total: 0.1760 s/iter. ETA=0:03:19\n",
      "\u001b[32m[09/06 16:11:50 d2.evaluation.evaluator]: \u001b[0mInference done 85/1199. Dataloading: 0.1125 s/iter. Inference: 0.0910 s/iter. Eval: 0.0008 s/iter. Total: 0.2045 s/iter. ETA=0:03:47\n",
      "\u001b[32m[09/06 16:11:56 d2.evaluation.evaluator]: \u001b[0mInference done 117/1199. Dataloading: 0.1013 s/iter. Inference: 0.0888 s/iter. Eval: 0.0008 s/iter. Total: 0.1910 s/iter. ETA=0:03:26\n",
      "\u001b[32m[09/06 16:12:01 d2.evaluation.evaluator]: \u001b[0mInference done 135/1199. Dataloading: 0.1149 s/iter. Inference: 0.0903 s/iter. Eval: 0.0008 s/iter. Total: 0.2061 s/iter. ETA=0:03:39\n",
      "\u001b[32m[09/06 16:12:06 d2.evaluation.evaluator]: \u001b[0mInference done 165/1199. Dataloading: 0.1084 s/iter. Inference: 0.0906 s/iter. Eval: 0.0008 s/iter. Total: 0.1999 s/iter. ETA=0:03:26\n",
      "\u001b[32m[09/06 16:12:11 d2.evaluation.evaluator]: \u001b[0mInference done 193/1199. Dataloading: 0.1082 s/iter. Inference: 0.0896 s/iter. Eval: 0.0008 s/iter. Total: 0.1987 s/iter. ETA=0:03:19\n",
      "\u001b[32m[09/06 16:12:17 d2.evaluation.evaluator]: \u001b[0mInference done 219/1199. Dataloading: 0.1079 s/iter. Inference: 0.0893 s/iter. Eval: 0.0008 s/iter. Total: 0.1981 s/iter. ETA=0:03:14\n",
      "\u001b[32m[09/06 16:12:22 d2.evaluation.evaluator]: \u001b[0mInference done 241/1199. Dataloading: 0.1120 s/iter. Inference: 0.0894 s/iter. Eval: 0.0008 s/iter. Total: 0.2023 s/iter. ETA=0:03:13\n",
      "\u001b[32m[09/06 16:12:27 d2.evaluation.evaluator]: \u001b[0mInference done 264/1199. Dataloading: 0.1134 s/iter. Inference: 0.0897 s/iter. Eval: 0.0008 s/iter. Total: 0.2041 s/iter. ETA=0:03:10\n",
      "\u001b[32m[09/06 16:12:33 d2.evaluation.evaluator]: \u001b[0mInference done 282/1199. Dataloading: 0.1229 s/iter. Inference: 0.0895 s/iter. Eval: 0.0008 s/iter. Total: 0.2133 s/iter. ETA=0:03:15\n",
      "\u001b[32m[09/06 16:12:38 d2.evaluation.evaluator]: \u001b[0mInference done 309/1199. Dataloading: 0.1208 s/iter. Inference: 0.0893 s/iter. Eval: 0.0008 s/iter. Total: 0.2110 s/iter. ETA=0:03:07\n",
      "\u001b[32m[09/06 16:12:43 d2.evaluation.evaluator]: \u001b[0mInference done 340/1199. Dataloading: 0.1179 s/iter. Inference: 0.0879 s/iter. Eval: 0.0008 s/iter. Total: 0.2067 s/iter. ETA=0:02:57\n",
      "\u001b[32m[09/06 16:12:48 d2.evaluation.evaluator]: \u001b[0mInference done 368/1199. Dataloading: 0.1173 s/iter. Inference: 0.0866 s/iter. Eval: 0.0008 s/iter. Total: 0.2048 s/iter. ETA=0:02:50\n",
      "\u001b[32m[09/06 16:12:54 d2.evaluation.evaluator]: \u001b[0mInference done 388/1199. Dataloading: 0.1195 s/iter. Inference: 0.0869 s/iter. Eval: 0.0007 s/iter. Total: 0.2073 s/iter. ETA=0:02:48\n",
      "\u001b[32m[09/06 16:12:59 d2.evaluation.evaluator]: \u001b[0mInference done 415/1199. Dataloading: 0.1184 s/iter. Inference: 0.0868 s/iter. Eval: 0.0007 s/iter. Total: 0.2061 s/iter. ETA=0:02:41\n",
      "\u001b[32m[09/06 16:13:04 d2.evaluation.evaluator]: \u001b[0mInference done 448/1199. Dataloading: 0.1158 s/iter. Inference: 0.0860 s/iter. Eval: 0.0007 s/iter. Total: 0.2026 s/iter. ETA=0:02:32\n",
      "\u001b[32m[09/06 16:13:09 d2.evaluation.evaluator]: \u001b[0mInference done 473/1199. Dataloading: 0.1160 s/iter. Inference: 0.0858 s/iter. Eval: 0.0007 s/iter. Total: 0.2027 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/06 16:13:14 d2.evaluation.evaluator]: \u001b[0mInference done 504/1199. Dataloading: 0.1138 s/iter. Inference: 0.0856 s/iter. Eval: 0.0007 s/iter. Total: 0.2002 s/iter. ETA=0:02:19\n",
      "\u001b[32m[09/06 16:13:19 d2.evaluation.evaluator]: \u001b[0mInference done 530/1199. Dataloading: 0.1132 s/iter. Inference: 0.0858 s/iter. Eval: 0.0007 s/iter. Total: 0.1999 s/iter. ETA=0:02:13\n",
      "\u001b[32m[09/06 16:13:24 d2.evaluation.evaluator]: \u001b[0mInference done 554/1199. Dataloading: 0.1140 s/iter. Inference: 0.0858 s/iter. Eval: 0.0007 s/iter. Total: 0.2006 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/06 16:13:30 d2.evaluation.evaluator]: \u001b[0mInference done 586/1199. Dataloading: 0.1128 s/iter. Inference: 0.0852 s/iter. Eval: 0.0007 s/iter. Total: 0.1989 s/iter. ETA=0:02:01\n",
      "\u001b[32m[09/06 16:13:35 d2.evaluation.evaluator]: \u001b[0mInference done 618/1199. Dataloading: 0.1117 s/iter. Inference: 0.0850 s/iter. Eval: 0.0007 s/iter. Total: 0.1975 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/06 16:13:40 d2.evaluation.evaluator]: \u001b[0mInference done 641/1199. Dataloading: 0.1124 s/iter. Inference: 0.0852 s/iter. Eval: 0.0007 s/iter. Total: 0.1985 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/06 16:13:45 d2.evaluation.evaluator]: \u001b[0mInference done 659/1199. Dataloading: 0.1136 s/iter. Inference: 0.0862 s/iter. Eval: 0.0007 s/iter. Total: 0.2007 s/iter. ETA=0:01:48\n",
      "\u001b[32m[09/06 16:13:50 d2.evaluation.evaluator]: \u001b[0mInference done 683/1199. Dataloading: 0.1140 s/iter. Inference: 0.0862 s/iter. Eval: 0.0007 s/iter. Total: 0.2010 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/06 16:13:56 d2.evaluation.evaluator]: \u001b[0mInference done 710/1199. Dataloading: 0.1133 s/iter. Inference: 0.0864 s/iter. Eval: 0.0007 s/iter. Total: 0.2006 s/iter. ETA=0:01:38\n",
      "\u001b[32m[09/06 16:14:01 d2.evaluation.evaluator]: \u001b[0mInference done 737/1199. Dataloading: 0.1128 s/iter. Inference: 0.0863 s/iter. Eval: 0.0007 s/iter. Total: 0.2000 s/iter. ETA=0:01:32\n",
      "\u001b[32m[09/06 16:14:06 d2.evaluation.evaluator]: \u001b[0mInference done 754/1199. Dataloading: 0.1150 s/iter. Inference: 0.0864 s/iter. Eval: 0.0007 s/iter. Total: 0.2022 s/iter. ETA=0:01:29\n",
      "\u001b[32m[09/06 16:14:11 d2.evaluation.evaluator]: \u001b[0mInference done 778/1199. Dataloading: 0.1152 s/iter. Inference: 0.0864 s/iter. Eval: 0.0007 s/iter. Total: 0.2024 s/iter. ETA=0:01:25\n",
      "\u001b[32m[09/06 16:14:16 d2.evaluation.evaluator]: \u001b[0mInference done 811/1199. Dataloading: 0.1136 s/iter. Inference: 0.0860 s/iter. Eval: 0.0007 s/iter. Total: 0.2004 s/iter. ETA=0:01:17\n",
      "\u001b[32m[09/06 16:14:21 d2.evaluation.evaluator]: \u001b[0mInference done 842/1199. Dataloading: 0.1123 s/iter. Inference: 0.0859 s/iter. Eval: 0.0007 s/iter. Total: 0.1991 s/iter. ETA=0:01:11\n",
      "\u001b[32m[09/06 16:14:26 d2.evaluation.evaluator]: \u001b[0mInference done 870/1199. Dataloading: 0.1119 s/iter. Inference: 0.0857 s/iter. Eval: 0.0007 s/iter. Total: 0.1985 s/iter. ETA=0:01:05\n",
      "\u001b[32m[09/06 16:14:31 d2.evaluation.evaluator]: \u001b[0mInference done 897/1199. Dataloading: 0.1120 s/iter. Inference: 0.0853 s/iter. Eval: 0.0007 s/iter. Total: 0.1982 s/iter. ETA=0:00:59\n",
      "\u001b[32m[09/06 16:14:36 d2.evaluation.evaluator]: \u001b[0mInference done 929/1199. Dataloading: 0.1113 s/iter. Inference: 0.0848 s/iter. Eval: 0.0007 s/iter. Total: 0.1970 s/iter. ETA=0:00:53\n",
      "\u001b[32m[09/06 16:14:41 d2.evaluation.evaluator]: \u001b[0mInference done 959/1199. Dataloading: 0.1106 s/iter. Inference: 0.0847 s/iter. Eval: 0.0007 s/iter. Total: 0.1961 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/06 16:14:47 d2.evaluation.evaluator]: \u001b[0mInference done 981/1199. Dataloading: 0.1116 s/iter. Inference: 0.0849 s/iter. Eval: 0.0007 s/iter. Total: 0.1973 s/iter. ETA=0:00:43\n",
      "\u001b[32m[09/06 16:14:52 d2.evaluation.evaluator]: \u001b[0mInference done 1011/1199. Dataloading: 0.1111 s/iter. Inference: 0.0846 s/iter. Eval: 0.0007 s/iter. Total: 0.1965 s/iter. ETA=0:00:36\n",
      "\u001b[32m[09/06 16:14:57 d2.evaluation.evaluator]: \u001b[0mInference done 1030/1199. Dataloading: 0.1121 s/iter. Inference: 0.0849 s/iter. Eval: 0.0007 s/iter. Total: 0.1979 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/06 16:15:02 d2.evaluation.evaluator]: \u001b[0mInference done 1058/1199. Dataloading: 0.1119 s/iter. Inference: 0.0848 s/iter. Eval: 0.0007 s/iter. Total: 0.1974 s/iter. ETA=0:00:27\n",
      "\u001b[32m[09/06 16:15:07 d2.evaluation.evaluator]: \u001b[0mInference done 1083/1199. Dataloading: 0.1121 s/iter. Inference: 0.0847 s/iter. Eval: 0.0007 s/iter. Total: 0.1976 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/06 16:15:13 d2.evaluation.evaluator]: \u001b[0mInference done 1113/1199. Dataloading: 0.1115 s/iter. Inference: 0.0848 s/iter. Eval: 0.0007 s/iter. Total: 0.1972 s/iter. ETA=0:00:16\n",
      "\u001b[32m[09/06 16:15:18 d2.evaluation.evaluator]: \u001b[0mInference done 1132/1199. Dataloading: 0.1126 s/iter. Inference: 0.0849 s/iter. Eval: 0.0007 s/iter. Total: 0.1984 s/iter. ETA=0:00:13\n",
      "\u001b[32m[09/06 16:15:23 d2.evaluation.evaluator]: \u001b[0mInference done 1158/1199. Dataloading: 0.1123 s/iter. Inference: 0.0852 s/iter. Eval: 0.0007 s/iter. Total: 0.1983 s/iter. ETA=0:00:08\n",
      "\u001b[32m[09/06 16:15:28 d2.evaluation.evaluator]: \u001b[0mInference done 1186/1199. Dataloading: 0.1119 s/iter. Inference: 0.0853 s/iter. Eval: 0.0007 s/iter. Total: 0.1981 s/iter. ETA=0:00:02\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:56.511674 (0.198083 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:41 (0.085186 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.22 seconds.\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.414\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.401\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.850\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 33.345 | 41.431 | 40.087 |  nan  |  nan  | 33.345 |\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 25.458 | sad        | 23.194 | surprised  | 35.239 |\n",
      "| happy      | 49.487 |            |        |            |        |\n",
      "\u001b[32m[09/06 16:15:31 d2.engine.defaults]: \u001b[0mEvaluation results for face_data_set_valid in csv format:\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.testing]: \u001b[0mcopypaste: 33.3447,41.4307,40.0869,nan,nan,33.3447\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 16:15:31 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 16:15:31 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 16:15:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 16:15:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 16:15:31 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 16:15:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/06 16:15:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 16:15:33 d2.evaluation.evaluator]: \u001b[0mInference done 13/1199. Dataloading: 0.0661 s/iter. Inference: 0.0806 s/iter. Eval: 0.0006 s/iter. Total: 0.1474 s/iter. ETA=0:02:54\n",
      "\u001b[32m[09/06 16:15:38 d2.evaluation.evaluator]: \u001b[0mInference done 41/1199. Dataloading: 0.0848 s/iter. Inference: 0.0875 s/iter. Eval: 0.0007 s/iter. Total: 0.1732 s/iter. ETA=0:03:20\n",
      "\u001b[32m[09/06 16:15:43 d2.evaluation.evaluator]: \u001b[0mInference done 71/1199. Dataloading: 0.0863 s/iter. Inference: 0.0846 s/iter. Eval: 0.0007 s/iter. Total: 0.1718 s/iter. ETA=0:03:13\n",
      "\u001b[32m[09/06 16:15:48 d2.evaluation.evaluator]: \u001b[0mInference done 93/1199. Dataloading: 0.0993 s/iter. Inference: 0.0864 s/iter. Eval: 0.0007 s/iter. Total: 0.1865 s/iter. ETA=0:03:26\n",
      "\u001b[32m[09/06 16:15:54 d2.evaluation.evaluator]: \u001b[0mInference done 119/1199. Dataloading: 0.1042 s/iter. Inference: 0.0866 s/iter. Eval: 0.0007 s/iter. Total: 0.1917 s/iter. ETA=0:03:26\n",
      "\u001b[32m[09/06 16:15:59 d2.evaluation.evaluator]: \u001b[0mInference done 152/1199. Dataloading: 0.0982 s/iter. Inference: 0.0847 s/iter. Eval: 0.0007 s/iter. Total: 0.1836 s/iter. ETA=0:03:12\n",
      "\u001b[32m[09/06 16:16:04 d2.evaluation.evaluator]: \u001b[0mInference done 181/1199. Dataloading: 0.0975 s/iter. Inference: 0.0839 s/iter. Eval: 0.0007 s/iter. Total: 0.1822 s/iter. ETA=0:03:05\n",
      "\u001b[32m[09/06 16:16:09 d2.evaluation.evaluator]: \u001b[0mInference done 212/1199. Dataloading: 0.0962 s/iter. Inference: 0.0822 s/iter. Eval: 0.0007 s/iter. Total: 0.1792 s/iter. ETA=0:02:56\n",
      "\u001b[32m[09/06 16:16:14 d2.evaluation.evaluator]: \u001b[0mInference done 237/1199. Dataloading: 0.0985 s/iter. Inference: 0.0827 s/iter. Eval: 0.0007 s/iter. Total: 0.1820 s/iter. ETA=0:02:55\n",
      "\u001b[32m[09/06 16:16:19 d2.evaluation.evaluator]: \u001b[0mInference done 262/1199. Dataloading: 0.1010 s/iter. Inference: 0.0824 s/iter. Eval: 0.0007 s/iter. Total: 0.1842 s/iter. ETA=0:02:52\n",
      "\u001b[32m[09/06 16:16:25 d2.evaluation.evaluator]: \u001b[0mInference done 282/1199. Dataloading: 0.1101 s/iter. Inference: 0.0817 s/iter. Eval: 0.0007 s/iter. Total: 0.1926 s/iter. ETA=0:02:56\n",
      "\u001b[32m[09/06 16:16:31 d2.evaluation.evaluator]: \u001b[0mInference done 313/1199. Dataloading: 0.1083 s/iter. Inference: 0.0814 s/iter. Eval: 0.0007 s/iter. Total: 0.1904 s/iter. ETA=0:02:48\n",
      "\u001b[32m[09/06 16:16:36 d2.evaluation.evaluator]: \u001b[0mInference done 344/1199. Dataloading: 0.1064 s/iter. Inference: 0.0808 s/iter. Eval: 0.0007 s/iter. Total: 0.1880 s/iter. ETA=0:02:40\n",
      "\u001b[32m[09/06 16:16:41 d2.evaluation.evaluator]: \u001b[0mInference done 366/1199. Dataloading: 0.1080 s/iter. Inference: 0.0827 s/iter. Eval: 0.0007 s/iter. Total: 0.1915 s/iter. ETA=0:02:39\n",
      "\u001b[32m[09/06 16:16:46 d2.evaluation.evaluator]: \u001b[0mInference done 388/1199. Dataloading: 0.1103 s/iter. Inference: 0.0831 s/iter. Eval: 0.0007 s/iter. Total: 0.1942 s/iter. ETA=0:02:37\n",
      "\u001b[32m[09/06 16:16:51 d2.evaluation.evaluator]: \u001b[0mInference done 413/1199. Dataloading: 0.1107 s/iter. Inference: 0.0831 s/iter. Eval: 0.0007 s/iter. Total: 0.1946 s/iter. ETA=0:02:32\n",
      "\u001b[32m[09/06 16:16:57 d2.evaluation.evaluator]: \u001b[0mInference done 442/1199. Dataloading: 0.1089 s/iter. Inference: 0.0837 s/iter. Eval: 0.0007 s/iter. Total: 0.1935 s/iter. ETA=0:02:26\n",
      "\u001b[32m[09/06 16:17:02 d2.evaluation.evaluator]: \u001b[0mInference done 471/1199. Dataloading: 0.1086 s/iter. Inference: 0.0835 s/iter. Eval: 0.0007 s/iter. Total: 0.1929 s/iter. ETA=0:02:20\n",
      "\u001b[32m[09/06 16:17:07 d2.evaluation.evaluator]: \u001b[0mInference done 504/1199. Dataloading: 0.1069 s/iter. Inference: 0.0827 s/iter. Eval: 0.0007 s/iter. Total: 0.1905 s/iter. ETA=0:02:12\n",
      "\u001b[32m[09/06 16:17:12 d2.evaluation.evaluator]: \u001b[0mInference done 534/1199. Dataloading: 0.1065 s/iter. Inference: 0.0821 s/iter. Eval: 0.0007 s/iter. Total: 0.1895 s/iter. ETA=0:02:06\n",
      "\u001b[32m[09/06 16:17:18 d2.evaluation.evaluator]: \u001b[0mInference done 566/1199. Dataloading: 0.1055 s/iter. Inference: 0.0820 s/iter. Eval: 0.0007 s/iter. Total: 0.1883 s/iter. ETA=0:01:59\n",
      "\u001b[32m[09/06 16:17:23 d2.evaluation.evaluator]: \u001b[0mInference done 592/1199. Dataloading: 0.1056 s/iter. Inference: 0.0822 s/iter. Eval: 0.0007 s/iter. Total: 0.1886 s/iter. ETA=0:01:54\n",
      "\u001b[32m[09/06 16:17:28 d2.evaluation.evaluator]: \u001b[0mInference done 626/1199. Dataloading: 0.1039 s/iter. Inference: 0.0821 s/iter. Eval: 0.0007 s/iter. Total: 0.1868 s/iter. ETA=0:01:47\n",
      "\u001b[32m[09/06 16:17:33 d2.evaluation.evaluator]: \u001b[0mInference done 648/1199. Dataloading: 0.1057 s/iter. Inference: 0.0818 s/iter. Eval: 0.0007 s/iter. Total: 0.1883 s/iter. ETA=0:01:43\n",
      "\u001b[32m[09/06 16:17:39 d2.evaluation.evaluator]: \u001b[0mInference done 675/1199. Dataloading: 0.1068 s/iter. Inference: 0.0820 s/iter. Eval: 0.0007 s/iter. Total: 0.1896 s/iter. ETA=0:01:39\n",
      "\u001b[32m[09/06 16:17:44 d2.evaluation.evaluator]: \u001b[0mInference done 706/1199. Dataloading: 0.1053 s/iter. Inference: 0.0824 s/iter. Eval: 0.0007 s/iter. Total: 0.1885 s/iter. ETA=0:01:32\n",
      "\u001b[32m[09/06 16:17:49 d2.evaluation.evaluator]: \u001b[0mInference done 734/1199. Dataloading: 0.1050 s/iter. Inference: 0.0823 s/iter. Eval: 0.0007 s/iter. Total: 0.1882 s/iter. ETA=0:01:27\n",
      "\u001b[32m[09/06 16:17:55 d2.evaluation.evaluator]: \u001b[0mInference done 752/1199. Dataloading: 0.1078 s/iter. Inference: 0.0824 s/iter. Eval: 0.0007 s/iter. Total: 0.1911 s/iter. ETA=0:01:25\n",
      "\u001b[32m[09/06 16:18:00 d2.evaluation.evaluator]: \u001b[0mInference done 780/1199. Dataloading: 0.1075 s/iter. Inference: 0.0823 s/iter. Eval: 0.0007 s/iter. Total: 0.1907 s/iter. ETA=0:01:19\n",
      "\u001b[32m[09/06 16:18:05 d2.evaluation.evaluator]: \u001b[0mInference done 813/1199. Dataloading: 0.1061 s/iter. Inference: 0.0822 s/iter. Eval: 0.0007 s/iter. Total: 0.1892 s/iter. ETA=0:01:13\n",
      "\u001b[32m[09/06 16:18:10 d2.evaluation.evaluator]: \u001b[0mInference done 846/1199. Dataloading: 0.1050 s/iter. Inference: 0.0819 s/iter. Eval: 0.0007 s/iter. Total: 0.1878 s/iter. ETA=0:01:06\n",
      "\u001b[32m[09/06 16:18:15 d2.evaluation.evaluator]: \u001b[0mInference done 875/1199. Dataloading: 0.1048 s/iter. Inference: 0.0819 s/iter. Eval: 0.0007 s/iter. Total: 0.1875 s/iter. ETA=0:01:00\n",
      "\u001b[32m[09/06 16:18:20 d2.evaluation.evaluator]: \u001b[0mInference done 902/1199. Dataloading: 0.1049 s/iter. Inference: 0.0818 s/iter. Eval: 0.0007 s/iter. Total: 0.1876 s/iter. ETA=0:00:55\n",
      "\u001b[32m[09/06 16:18:25 d2.evaluation.evaluator]: \u001b[0mInference done 930/1199. Dataloading: 0.1042 s/iter. Inference: 0.0823 s/iter. Eval: 0.0007 s/iter. Total: 0.1873 s/iter. ETA=0:00:50\n",
      "\u001b[32m[09/06 16:18:30 d2.evaluation.evaluator]: \u001b[0mInference done 960/1199. Dataloading: 0.1035 s/iter. Inference: 0.0824 s/iter. Eval: 0.0007 s/iter. Total: 0.1867 s/iter. ETA=0:00:44\n",
      "\u001b[32m[09/06 16:18:35 d2.evaluation.evaluator]: \u001b[0mInference done 981/1199. Dataloading: 0.1045 s/iter. Inference: 0.0825 s/iter. Eval: 0.0007 s/iter. Total: 0.1879 s/iter. ETA=0:00:40\n",
      "\u001b[32m[09/06 16:18:41 d2.evaluation.evaluator]: \u001b[0mInference done 1009/1199. Dataloading: 0.1044 s/iter. Inference: 0.0825 s/iter. Eval: 0.0007 s/iter. Total: 0.1878 s/iter. ETA=0:00:35\n",
      "\u001b[32m[09/06 16:18:46 d2.evaluation.evaluator]: \u001b[0mInference done 1030/1199. Dataloading: 0.1052 s/iter. Inference: 0.0828 s/iter. Eval: 0.0007 s/iter. Total: 0.1889 s/iter. ETA=0:00:31\n",
      "\u001b[32m[09/06 16:18:51 d2.evaluation.evaluator]: \u001b[0mInference done 1057/1199. Dataloading: 0.1052 s/iter. Inference: 0.0830 s/iter. Eval: 0.0007 s/iter. Total: 0.1890 s/iter. ETA=0:00:26\n",
      "\u001b[32m[09/06 16:18:56 d2.evaluation.evaluator]: \u001b[0mInference done 1085/1199. Dataloading: 0.1054 s/iter. Inference: 0.0829 s/iter. Eval: 0.0007 s/iter. Total: 0.1891 s/iter. ETA=0:00:21\n",
      "\u001b[32m[09/06 16:19:01 d2.evaluation.evaluator]: \u001b[0mInference done 1115/1199. Dataloading: 0.1049 s/iter. Inference: 0.0828 s/iter. Eval: 0.0007 s/iter. Total: 0.1886 s/iter. ETA=0:00:15\n",
      "\u001b[32m[09/06 16:19:06 d2.evaluation.evaluator]: \u001b[0mInference done 1140/1199. Dataloading: 0.1054 s/iter. Inference: 0.0826 s/iter. Eval: 0.0007 s/iter. Total: 0.1888 s/iter. ETA=0:00:11\n",
      "\u001b[32m[09/06 16:19:11 d2.evaluation.evaluator]: \u001b[0mInference done 1166/1199. Dataloading: 0.1055 s/iter. Inference: 0.0826 s/iter. Eval: 0.0007 s/iter. Total: 0.1889 s/iter. ETA=0:00:06\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.evaluator]: \u001b[0mInference done 1198/1199. Dataloading: 0.1051 s/iter. Inference: 0.0823 s/iter. Eval: 0.0007 s/iter. Total: 0.1883 s/iter. ETA=0:00:00\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:44.698038 (0.188189 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:38 (0.082316 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.21 seconds.\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.414\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.401\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.850\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 33.345 | 41.431 | 40.087 |  nan  |  nan  | 33.345 |\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 16:19:17 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 25.458 | sad        | 23.194 | surprised  | 35.239 |\n",
      "| happy      | 49.487 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 33.34470185876188, 'AP50': 41.43071684067699, 'AP75': 40.08687110954399, 'APs': nan, 'APm': nan, 'APl': 33.34470185876188, 'AP-angry': 25.4582152211551, 'AP-sad': 23.19429527856751, 'AP-surprised': 35.23905012399878, 'AP-happy': 49.487246811326145})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[09/06 16:19:20 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[09/06 16:19:23 d2.data.datasets.coco]: \u001b[0mLoading data/ssd_rcnn_face/annotations_val.json takes 3.16 seconds.\n",
      "\u001b[32m[09/06 16:19:23 d2.data.datasets.coco]: \u001b[0mLoaded 1199 images in COCO format from data/ssd_rcnn_face/annotations_val.json\n",
      "\u001b[32m[09/06 16:19:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(512, 512), max_size=512, sample_style='choice')]\n",
      "\u001b[32m[09/06 16:19:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[09/06 16:19:23 d2.data.common]: \u001b[0mSerializing 1199 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[09/06 16:19:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.36 MiB\n",
      "\u001b[32m[09/06 16:19:23 d2.evaluation.evaluator]: \u001b[0mStart inference on 1199 batches\n",
      "\u001b[32m[09/06 16:19:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/1199. Dataloading: 0.0459 s/iter. Inference: 0.0772 s/iter. Eval: 0.0007 s/iter. Total: 0.1238 s/iter. ETA=0:02:27\n",
      "\u001b[32m[09/06 16:19:30 d2.evaluation.evaluator]: \u001b[0mInference done 38/1199. Dataloading: 0.0920 s/iter. Inference: 0.0821 s/iter. Eval: 0.0008 s/iter. Total: 0.1751 s/iter. ETA=0:03:23\n",
      "\u001b[32m[09/06 16:19:35 d2.evaluation.evaluator]: \u001b[0mInference done 69/1199. Dataloading: 0.0878 s/iter. Inference: 0.0800 s/iter. Eval: 0.0007 s/iter. Total: 0.1687 s/iter. ETA=0:03:10\n",
      "\u001b[32m[09/06 16:19:40 d2.evaluation.evaluator]: \u001b[0mInference done 87/1199. Dataloading: 0.1081 s/iter. Inference: 0.0841 s/iter. Eval: 0.0008 s/iter. Total: 0.1931 s/iter. ETA=0:03:34\n",
      "\u001b[32m[09/06 16:19:46 d2.evaluation.evaluator]: \u001b[0mInference done 117/1199. Dataloading: 0.1004 s/iter. Inference: 0.0864 s/iter. Eval: 0.0007 s/iter. Total: 0.1877 s/iter. ETA=0:03:23\n",
      "\u001b[32m[09/06 16:19:51 d2.evaluation.evaluator]: \u001b[0mInference done 139/1199. Dataloading: 0.1081 s/iter. Inference: 0.0854 s/iter. Eval: 0.0007 s/iter. Total: 0.1944 s/iter. ETA=0:03:26\n",
      "\u001b[32m[09/06 16:19:56 d2.evaluation.evaluator]: \u001b[0mInference done 173/1199. Dataloading: 0.1032 s/iter. Inference: 0.0828 s/iter. Eval: 0.0007 s/iter. Total: 0.1869 s/iter. ETA=0:03:11\n",
      "\u001b[32m[09/06 16:20:01 d2.evaluation.evaluator]: \u001b[0mInference done 201/1199. Dataloading: 0.1025 s/iter. Inference: 0.0828 s/iter. Eval: 0.0007 s/iter. Total: 0.1862 s/iter. ETA=0:03:05\n",
      "\u001b[32m[09/06 16:20:07 d2.evaluation.evaluator]: \u001b[0mInference done 227/1199. Dataloading: 0.1045 s/iter. Inference: 0.0840 s/iter. Eval: 0.0008 s/iter. Total: 0.1894 s/iter. ETA=0:03:04\n",
      "\u001b[32m[09/06 16:20:12 d2.evaluation.evaluator]: \u001b[0mInference done 252/1199. Dataloading: 0.1063 s/iter. Inference: 0.0836 s/iter. Eval: 0.0007 s/iter. Total: 0.1908 s/iter. ETA=0:03:00\n",
      "\u001b[32m[09/06 16:20:17 d2.evaluation.evaluator]: \u001b[0mInference done 276/1199. Dataloading: 0.1095 s/iter. Inference: 0.0833 s/iter. Eval: 0.0007 s/iter. Total: 0.1937 s/iter. ETA=0:02:58\n",
      "\u001b[32m[09/06 16:20:23 d2.evaluation.evaluator]: \u001b[0mInference done 288/1199. Dataloading: 0.1196 s/iter. Inference: 0.0845 s/iter. Eval: 0.0008 s/iter. Total: 0.2050 s/iter. ETA=0:03:06\n",
      "\u001b[32m[09/06 16:20:28 d2.evaluation.evaluator]: \u001b[0mInference done 314/1199. Dataloading: 0.1190 s/iter. Inference: 0.0842 s/iter. Eval: 0.0008 s/iter. Total: 0.2041 s/iter. ETA=0:03:00\n",
      "\u001b[32m[09/06 16:20:33 d2.evaluation.evaluator]: \u001b[0mInference done 344/1199. Dataloading: 0.1164 s/iter. Inference: 0.0838 s/iter. Eval: 0.0008 s/iter. Total: 0.2011 s/iter. ETA=0:02:51\n",
      "\u001b[32m[09/06 16:20:38 d2.evaluation.evaluator]: \u001b[0mInference done 373/1199. Dataloading: 0.1155 s/iter. Inference: 0.0826 s/iter. Eval: 0.0007 s/iter. Total: 0.1990 s/iter. ETA=0:02:44\n",
      "\u001b[32m[09/06 16:20:43 d2.evaluation.evaluator]: \u001b[0mInference done 400/1199. Dataloading: 0.1155 s/iter. Inference: 0.0818 s/iter. Eval: 0.0007 s/iter. Total: 0.1982 s/iter. ETA=0:02:38\n",
      "\u001b[32m[09/06 16:20:48 d2.evaluation.evaluator]: \u001b[0mInference done 429/1199. Dataloading: 0.1145 s/iter. Inference: 0.0815 s/iter. Eval: 0.0007 s/iter. Total: 0.1969 s/iter. ETA=0:02:31\n",
      "\u001b[32m[09/06 16:20:53 d2.evaluation.evaluator]: \u001b[0mInference done 463/1199. Dataloading: 0.1117 s/iter. Inference: 0.0812 s/iter. Eval: 0.0007 s/iter. Total: 0.1937 s/iter. ETA=0:02:22\n",
      "\u001b[32m[09/06 16:20:59 d2.evaluation.evaluator]: \u001b[0mInference done 492/1199. Dataloading: 0.1107 s/iter. Inference: 0.0812 s/iter. Eval: 0.0007 s/iter. Total: 0.1928 s/iter. ETA=0:02:16\n",
      "\u001b[32m[09/06 16:21:04 d2.evaluation.evaluator]: \u001b[0mInference done 521/1199. Dataloading: 0.1098 s/iter. Inference: 0.0810 s/iter. Eval: 0.0007 s/iter. Total: 0.1917 s/iter. ETA=0:02:09\n",
      "\u001b[32m[09/06 16:21:09 d2.evaluation.evaluator]: \u001b[0mInference done 550/1199. Dataloading: 0.1092 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1907 s/iter. ETA=0:02:03\n",
      "\u001b[32m[09/06 16:21:14 d2.evaluation.evaluator]: \u001b[0mInference done 580/1199. Dataloading: 0.1080 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1895 s/iter. ETA=0:01:57\n",
      "\u001b[32m[09/06 16:21:19 d2.evaluation.evaluator]: \u001b[0mInference done 611/1199. Dataloading: 0.1068 s/iter. Inference: 0.0806 s/iter. Eval: 0.0007 s/iter. Total: 0.1883 s/iter. ETA=0:01:50\n",
      "\u001b[32m[09/06 16:21:24 d2.evaluation.evaluator]: \u001b[0mInference done 638/1199. Dataloading: 0.1070 s/iter. Inference: 0.0806 s/iter. Eval: 0.0007 s/iter. Total: 0.1885 s/iter. ETA=0:01:45\n",
      "\u001b[32m[09/06 16:21:29 d2.evaluation.evaluator]: \u001b[0mInference done 662/1199. Dataloading: 0.1078 s/iter. Inference: 0.0806 s/iter. Eval: 0.0007 s/iter. Total: 0.1893 s/iter. ETA=0:01:41\n",
      "\u001b[32m[09/06 16:21:34 d2.evaluation.evaluator]: \u001b[0mInference done 689/1199. Dataloading: 0.1077 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1892 s/iter. ETA=0:01:36\n",
      "\u001b[32m[09/06 16:21:39 d2.evaluation.evaluator]: \u001b[0mInference done 720/1199. Dataloading: 0.1068 s/iter. Inference: 0.0805 s/iter. Eval: 0.0007 s/iter. Total: 0.1882 s/iter. ETA=0:01:30\n",
      "\u001b[32m[09/06 16:21:45 d2.evaluation.evaluator]: \u001b[0mInference done 747/1199. Dataloading: 0.1078 s/iter. Inference: 0.0804 s/iter. Eval: 0.0007 s/iter. Total: 0.1891 s/iter. ETA=0:01:25\n",
      "\u001b[32m[09/06 16:21:50 d2.evaluation.evaluator]: \u001b[0mInference done 764/1199. Dataloading: 0.1095 s/iter. Inference: 0.0810 s/iter. Eval: 0.0007 s/iter. Total: 0.1914 s/iter. ETA=0:01:23\n",
      "\u001b[32m[09/06 16:21:55 d2.evaluation.evaluator]: \u001b[0mInference done 795/1199. Dataloading: 0.1085 s/iter. Inference: 0.0809 s/iter. Eval: 0.0007 s/iter. Total: 0.1903 s/iter. ETA=0:01:16\n",
      "\u001b[32m[09/06 16:22:00 d2.evaluation.evaluator]: \u001b[0mInference done 827/1199. Dataloading: 0.1075 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1890 s/iter. ETA=0:01:10\n",
      "\u001b[32m[09/06 16:22:05 d2.evaluation.evaluator]: \u001b[0mInference done 856/1199. Dataloading: 0.1070 s/iter. Inference: 0.0811 s/iter. Eval: 0.0007 s/iter. Total: 0.1889 s/iter. ETA=0:01:04\n",
      "\u001b[32m[09/06 16:22:10 d2.evaluation.evaluator]: \u001b[0mInference done 885/1199. Dataloading: 0.1068 s/iter. Inference: 0.0808 s/iter. Eval: 0.0007 s/iter. Total: 0.1884 s/iter. ETA=0:00:59\n",
      "\u001b[32m[09/06 16:22:16 d2.evaluation.evaluator]: \u001b[0mInference done 911/1199. Dataloading: 0.1070 s/iter. Inference: 0.0808 s/iter. Eval: 0.0007 s/iter. Total: 0.1887 s/iter. ETA=0:00:54\n",
      "\u001b[32m[09/06 16:22:21 d2.evaluation.evaluator]: \u001b[0mInference done 945/1199. Dataloading: 0.1059 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1874 s/iter. ETA=0:00:47\n",
      "\u001b[32m[09/06 16:22:26 d2.evaluation.evaluator]: \u001b[0mInference done 971/1199. Dataloading: 0.1064 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1880 s/iter. ETA=0:00:42\n",
      "\u001b[32m[09/06 16:22:31 d2.evaluation.evaluator]: \u001b[0mInference done 998/1199. Dataloading: 0.1064 s/iter. Inference: 0.0807 s/iter. Eval: 0.0007 s/iter. Total: 0.1879 s/iter. ETA=0:00:37\n",
      "\u001b[32m[09/06 16:22:36 d2.evaluation.evaluator]: \u001b[0mInference done 1023/1199. Dataloading: 0.1069 s/iter. Inference: 0.0805 s/iter. Eval: 0.0007 s/iter. Total: 0.1882 s/iter. ETA=0:00:33\n",
      "\u001b[32m[09/06 16:22:41 d2.evaluation.evaluator]: \u001b[0mInference done 1050/1199. Dataloading: 0.1068 s/iter. Inference: 0.0806 s/iter. Eval: 0.0007 s/iter. Total: 0.1882 s/iter. ETA=0:00:28\n",
      "\u001b[32m[09/06 16:22:47 d2.evaluation.evaluator]: \u001b[0mInference done 1079/1199. Dataloading: 0.1068 s/iter. Inference: 0.0804 s/iter. Eval: 0.0007 s/iter. Total: 0.1880 s/iter. ETA=0:00:22\n",
      "\u001b[32m[09/06 16:22:52 d2.evaluation.evaluator]: \u001b[0mInference done 1108/1199. Dataloading: 0.1064 s/iter. Inference: 0.0804 s/iter. Eval: 0.0007 s/iter. Total: 0.1876 s/iter. ETA=0:00:17\n",
      "\u001b[32m[09/06 16:22:57 d2.evaluation.evaluator]: \u001b[0mInference done 1132/1199. Dataloading: 0.1072 s/iter. Inference: 0.0802 s/iter. Eval: 0.0007 s/iter. Total: 0.1882 s/iter. ETA=0:00:12\n",
      "\u001b[32m[09/06 16:23:02 d2.evaluation.evaluator]: \u001b[0mInference done 1158/1199. Dataloading: 0.1070 s/iter. Inference: 0.0806 s/iter. Eval: 0.0007 s/iter. Total: 0.1885 s/iter. ETA=0:00:07\n",
      "\u001b[32m[09/06 16:23:07 d2.evaluation.evaluator]: \u001b[0mInference done 1190/1199. Dataloading: 0.1064 s/iter. Inference: 0.0804 s/iter. Eval: 0.0007 s/iter. Total: 0.1876 s/iter. ETA=0:00:01\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:44.088690 (0.187679 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:35 (0.080316 s / iter per device, on 1 devices)\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./models/faster_rcnn_R_50_FPN_3x/coco_instances_results.json\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.16 seconds.\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.333\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.414\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.401\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.850\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 33.345 | 41.431 | 40.087 |  nan  |  nan  | 33.345 |\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[09/06 16:23:09 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP     | category   | AP     | category   | AP     |\n",
      "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
      "| angry      | 25.458 | sad        | 23.194 | surprised  | 35.239 |\n",
      "| happy      | 49.487 |            |        |            |        |\n",
      "Evaluation results: OrderedDict([('bbox', {'AP': 33.34470185876188, 'AP50': 41.43071684067699, 'AP75': 40.08687110954399, 'APs': nan, 'APm': nan, 'APl': 33.34470185876188, 'AP-angry': 25.4582152211551, 'AP-sad': 23.19429527856751, 'AP-surprised': 35.23905012399878, 'AP-happy': 49.487246811326145})])\n",
      "Saving results to: ./models/faster_rcnn_R_50_FPN_3x/train_test_results.json\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import CfgNode as CN\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_dir=None):\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_dir)\n",
    "    \n",
    "    def train(self):\n",
    "        self.results = {\"train\": [], \"test\": []}\n",
    "        super().train()\n",
    "        \n",
    "        # Add evaluation after each epoch\n",
    "        for epoch in range(0, self.cfg.SOLVER.MAX_ITER + 1, self.cfg.TEST.EVAL_PERIOD):\n",
    "            self.do_test(epoch)\n",
    "            if epoch % self.cfg.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "                self.checkpointer.save(f\"model_{epoch}\")\n",
    "\n",
    "    def do_test(self, epoch):\n",
    "        try:\n",
    "            # Perform evaluation\n",
    "            evaluator = self.build_evaluator(self.cfg, self.cfg.DATASETS.TEST[0], output_dir=self.cfg.OUTPUT_DIR)\n",
    "            val_loader = build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0])\n",
    "            results = inference_on_dataset(self.model, val_loader, evaluator)\n",
    "            self.results[\"test\"].append({\"epoch\": epoch, \"results\": results})\n",
    "            print(\"Evaluation results:\", results)\n",
    "    \n",
    "            # Save the results as JSON\n",
    "            self.save_results_as_json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation at epoch {epoch}: {e}\")\n",
    "    \n",
    "    def save_results_as_json(self):\n",
    "        try:\n",
    "            output_file_path = os.path.join(self.cfg.OUTPUT_DIR, \"train_test_results.json\")\n",
    "            print(f\"Saving results to: {output_file_path}\")\n",
    "            with open(output_file_path, 'w') as f:\n",
    "                json.dump(self.results, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results as JSON: {e}\")\n",
    "\n",
    "\n",
    "# Set up configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"detectron2/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"face_data_set\",)\n",
    "cfg.DATASETS.TEST = (\"face_data_set_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 600000\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "cfg.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "cfg.INPUT.RANDOM_ROTATION = 30\n",
    "cfg.INPUT.CROP = CN({\"ENABLED\": True, \"TYPE\": \"relative_range\", \"SIZE\": [0.8, 0.8]})\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR = \"./models/faster_rcnn_R_50_FPN_3x\"\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 500\n",
    "\n",
    "setup_logger()\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
